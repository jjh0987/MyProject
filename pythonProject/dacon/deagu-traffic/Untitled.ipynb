{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb332d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import holidays\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "korea_holidays = holidays.SouthKorea(years=2023)\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f3a5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ECLO'\n",
    "light_df = pd.read_csv('./open/external_open/대구 보안등 정보.csv', encoding='cp949')[['설치개수', '소재지지번주소']]\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "light_df[['도시', '구', '동', '번지']] = light_df['소재지지번주소'].str.extract(location_pattern)\n",
    "light_df = light_df.drop(columns=['소재지지번주소', '번지'])\n",
    "light_df = light_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "light_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "child_area_df = pd.read_csv('./open/external_open/대구 어린이 보호 구역 정보.csv', encoding='cp949').drop_duplicates()# [['소재지지번주소']]\n",
    "child_area_df['시설수'] = 1\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "child_area_df[['도시', '구', '동', '번지']] = child_area_df['소재지지번주소'].str.extract(location_pattern)\n",
    "child_area_df = child_area_df.drop(columns=['소재지지번주소', '번지','위도','경도'])\n",
    "child_area_df = child_area_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "child_area_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "parking_df = pd.read_csv('./open/external_open/대구 주차장 정보.csv', encoding='cp949')\n",
    "parking_df = pd.get_dummies(parking_df, columns=['급지구분'])\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "parking_df[['도시', '구', '동', '번지']] = parking_df['소재지지번주소'].str.extract(location_pattern)\n",
    "parking_df = parking_df.drop(columns=['소재지지번주소', '번지','위도','경도'])\n",
    "parking_df = parking_df.groupby(['도시', '구', '동']).mean().reset_index()\n",
    "parking_df.reset_index(inplace=True, drop=True)\n",
    "parking_df = parking_df.fillna(0)\n",
    "\n",
    "train = pd.read_csv('./open/train.csv')\n",
    "test = pd.read_csv('./open/test.csv')\n",
    "cols = list(test.columns)\n",
    "cols.append(target)\n",
    "train = train[cols]\n",
    "\n",
    "time_pattern = r'(\\d{4})-(\\d{1,2})-(\\d{1,2}) (\\d{1,2})' \n",
    "train[['연', '월', '일', '시간']] = train['사고일시'].str.extract(time_pattern)\n",
    "train[['연', '월', '일', '시간']] = train[['연', '월', '일', '시간']].apply(pd.to_numeric) \n",
    "train = train.drop(columns=['사고일시'])\n",
    "test[['연', '월', '일', '시간']] = test['사고일시'].str.extract(time_pattern)\n",
    "test[['연', '월', '일', '시간']] = test[['연', '월', '일', '시간']].apply(pd.to_numeric)\n",
    "test = test.drop(columns=['사고일시'])\n",
    "\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+)'\n",
    "train[['도시', '구', '동']] = train['시군구'].str.extract(location_pattern)\n",
    "train = train.drop(columns=['시군구'])\n",
    "test[['도시', '구', '동']] = test['시군구'].str.extract(location_pattern)\n",
    "test = test.drop(columns=['시군구'])\n",
    "\n",
    "road_pattern = r'(.+) - (.+)'\n",
    "train[['도로형태1', '도로형태2']] = train['도로형태'].str.extract(road_pattern)\n",
    "train = train.drop(columns=['도로형태'])\n",
    "\n",
    "test[['도로형태1', '도로형태2']] = test['도로형태'].str.extract(road_pattern)\n",
    "test = test.drop(columns=['도로형태'])\n",
    "\n",
    "train = pd.merge(train, light_df, how='left', on=['도시', '구', '동'])\n",
    "train = pd.merge(train, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "train = pd.merge(train, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "test = pd.merge(test, light_df, how='left', on=['도시', '구', '동'])\n",
    "test = pd.merge(test, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "test = pd.merge(test, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "train = train.drop('ID',axis=1)\n",
    "test = test.drop('ID',axis=1)\n",
    "# train = train.dropna()\n",
    "\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "# test_x = test.copy()\n",
    "# train_x = train[test.columns].copy()\n",
    "# train_y = train['ECLO'].copy()\n",
    "categorical_features = list(train.dtypes[train.dtypes == \"object\"].index)\n",
    "for i in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train[i] = le.fit_transform(train[i])\n",
    "    test[i] = le.transform(test[i])\n",
    "    \n",
    "# minmax_scaler = MinMaxScaler()\n",
    "# data = minmax_scaler.fit_transform(train.drop('ECLO',axis=1))\n",
    "# data = pd.DataFrame(data,columns=train.drop('ECLO',axis=1).columns)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c206c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Batch 0: Invalid loss, terminating training\n",
      "   1/1981 [..............................] - ETA: 0s - loss: nan - rmsle: nanBatch 1: Invalid loss, terminating training\n",
      "Batch 2: Invalid loss, terminating training\n",
      "Batch 3: Invalid loss, terminating training\n",
      "Batch 4: Invalid loss, terminating training\n",
      "Batch 5: Invalid loss, terminating training\n",
      "Batch 6: Invalid loss, terminating training\n",
      "Batch 7: Invalid loss, terminating training\n",
      "Batch 8: Invalid loss, terminating training\n",
      "Batch 9: Invalid loss, terminating training\n",
      "Batch 10: Invalid loss, terminating training\n",
      "Batch 11: Invalid loss, terminating training\n",
      "Batch 12: Invalid loss, terminating training\n",
      "Batch 13: Invalid loss, terminating training\n",
      "Batch 14: Invalid loss, terminating training\n",
      "Batch 15: Invalid loss, terminating training\n",
      "Batch 16: Invalid loss, terminating training\n",
      "Batch 17: Invalid loss, terminating training\n",
      "Batch 18: Invalid loss, terminating training\n",
      "Batch 19: Invalid loss, terminating training\n",
      "Batch 20: Invalid loss, terminating training\n",
      "Batch 21: Invalid loss, terminating training\n",
      "Batch 22: Invalid loss, terminating training\n",
      "Batch 23: Invalid loss, terminating training\n",
      "Batch 24: Invalid loss, terminating training\n",
      "Batch 25: Invalid loss, terminating training\n",
      "Batch 26: Invalid loss, terminating training\n",
      "Batch 27: Invalid loss, terminating training\n",
      "Batch 28: Invalid loss, terminating training\n",
      "Batch 29: Invalid loss, terminating training\n",
      "Batch 30: Invalid loss, terminating training\n",
      "Batch 31: Invalid loss, terminating training\n",
      "Batch 32: Invalid loss, terminating training\n",
      "Batch 33: Invalid loss, terminating training\n",
      "Batch 34: Invalid loss, terminating training\n",
      "Batch 35: Invalid loss, terminating training\n",
      "Batch 36: Invalid loss, terminating training\n",
      "Batch 37: Invalid loss, terminating training\n",
      "Batch 38: Invalid loss, terminating training\n",
      "Batch 39: Invalid loss, terminating training\n",
      "Batch 40: Invalid loss, terminating training\n",
      "Batch 41: Invalid loss, terminating training\n",
      "Batch 42: Invalid loss, terminating training\n",
      "Batch 43: Invalid loss, terminating training\n",
      "Batch 44: Invalid loss, terminating training\n",
      "Batch 45: Invalid loss, terminating training\n",
      "Batch 46: Invalid loss, terminating training\n",
      "  47/1981 [..............................] - ETA: 2s - loss: nan - rmsle: nanBatch 47: Invalid loss, terminating training\n",
      "Batch 48: Invalid loss, terminating training\n",
      "Batch 49: Invalid loss, terminating training\n",
      "Batch 50: Invalid loss, terminating training\n",
      "Batch 51: Invalid loss, terminating training\n",
      "Batch 52: Invalid loss, terminating training\n",
      "Batch 53: Invalid loss, terminating training\n",
      "Batch 54: Invalid loss, terminating training\n",
      "Batch 55: Invalid loss, terminating training\n",
      "Batch 56: Invalid loss, terminating training\n",
      "Batch 57: Invalid loss, terminating training\n",
      "Batch 58: Invalid loss, terminating training\n",
      "Batch 59: Invalid loss, terminating training\n",
      "Batch 60: Invalid loss, terminating training\n",
      "Batch 61: Invalid loss, terminating training\n",
      "Batch 62: Invalid loss, terminating training\n",
      "Batch 63: Invalid loss, terminating training\n",
      "Batch 64: Invalid loss, terminating training\n",
      "Batch 65: Invalid loss, terminating training\n",
      "Batch 66: Invalid loss, terminating training\n",
      "Batch 67: Invalid loss, terminating training\n",
      "Batch 68: Invalid loss, terminating training\n",
      "Batch 69: Invalid loss, terminating training\n",
      "Batch 70: Invalid loss, terminating training\n",
      "Batch 71: Invalid loss, terminating training\n",
      "Batch 72: Invalid loss, terminating training\n",
      "Batch 73: Invalid loss, terminating training\n",
      "Batch 74: Invalid loss, terminating training\n",
      "Batch 75: Invalid loss, terminating training\n",
      "Batch 76: Invalid loss, terminating training\n",
      "Batch 77: Invalid loss, terminating training\n",
      "Batch 78: Invalid loss, terminating training\n",
      "Batch 79: Invalid loss, terminating training\n",
      "Batch 80: Invalid loss, terminating training\n",
      "Batch 81: Invalid loss, terminating training\n",
      "Batch 82: Invalid loss, terminating training\n",
      "Batch 83: Invalid loss, terminating training\n",
      "Batch 84: Invalid loss, terminating training\n",
      "Batch 85: Invalid loss, terminating training\n",
      "Batch 86: Invalid loss, terminating training\n",
      "Batch 87: Invalid loss, terminating training\n",
      "Batch 88: Invalid loss, terminating training\n",
      "Batch 89: Invalid loss, terminating training\n",
      "Batch 90: Invalid loss, terminating training\n",
      "Batch 91: Invalid loss, terminating training\n",
      "Batch 92: Invalid loss, terminating training\n",
      "Batch 93: Invalid loss, terminating training\n",
      "Batch 94: Invalid loss, terminating training\n",
      "Batch 95: Invalid loss, terminating training\n",
      "Batch 96: Invalid loss, terminating training\n",
      "Batch 97: Invalid loss, terminating training\n",
      "Batch 98: Invalid loss, terminating training\n",
      "Batch 99: Invalid loss, terminating training\n",
      "Batch 100: Invalid loss, terminating training\n",
      "Batch 101: Invalid loss, terminating training\n",
      "Batch 102: Invalid loss, terminating training\n",
      "Batch 103: Invalid loss, terminating training\n",
      "Batch 104: Invalid loss, terminating training\n",
      "Batch 105: Invalid loss, terminating training\n",
      "Batch 106: Invalid loss, terminating training\n",
      "Batch 107: Invalid loss, terminating training\n",
      "Batch 108: Invalid loss, terminating training\n",
      "Batch 109: Invalid loss, terminating training\n",
      "Batch 110: Invalid loss, terminating training\n",
      "Batch 111: Invalid loss, terminating training\n",
      "Batch 112: Invalid loss, terminating training\n",
      "Batch 113: Invalid loss, terminating training\n",
      "Batch 114: Invalid loss, terminating training\n",
      "Batch 115: Invalid loss, terminating training\n",
      "Batch 116: Invalid loss, terminating training\n",
      "Batch 117: Invalid loss, terminating training\n",
      "Batch 118: Invalid loss, terminating training\n",
      "Batch 119: Invalid loss, terminating training\n",
      "Batch 120: Invalid loss, terminating training\n",
      "Batch 121: Invalid loss, terminating training\n",
      " 122/1981 [>.............................] - ETA: 1s - loss: nan - rmsle: nanBatch 122: Invalid loss, terminating training\n",
      "Batch 123: Invalid loss, terminating training\n",
      "Batch 124: Invalid loss, terminating training\n",
      "Batch 125: Invalid loss, terminating training\n",
      "Batch 126: Invalid loss, terminating training\n",
      "Batch 127: Invalid loss, terminating training\n",
      "Batch 128: Invalid loss, terminating training\n",
      "Batch 129: Invalid loss, terminating training\n",
      "Batch 130: Invalid loss, terminating training\n",
      "Batch 131: Invalid loss, terminating training\n",
      "Batch 132: Invalid loss, terminating training\n",
      "Batch 133: Invalid loss, terminating training\n",
      "Batch 134: Invalid loss, terminating training\n",
      "Batch 135: Invalid loss, terminating training\n",
      "Batch 136: Invalid loss, terminating training\n",
      "Batch 137: Invalid loss, terminating training\n",
      "Batch 138: Invalid loss, terminating training\n",
      "Batch 139: Invalid loss, terminating training\n",
      "Batch 140: Invalid loss, terminating training\n",
      "Batch 141: Invalid loss, terminating training\n",
      "Batch 142: Invalid loss, terminating training\n",
      "Batch 143: Invalid loss, terminating training\n",
      "Batch 144: Invalid loss, terminating training\n",
      "Batch 145: Invalid loss, terminating training\n",
      "Batch 146: Invalid loss, terminating training\n",
      "Batch 147: Invalid loss, terminating training\n",
      "Batch 148: Invalid loss, terminating training\n",
      "Batch 149: Invalid loss, terminating training\n",
      "Batch 150: Invalid loss, terminating training\n",
      "Batch 151: Invalid loss, terminating training\n",
      "Batch 152: Invalid loss, terminating training\n",
      "Batch 153: Invalid loss, terminating training\n",
      "Batch 154: Invalid loss, terminating training\n",
      "Batch 155: Invalid loss, terminating training\n",
      "Batch 156: Invalid loss, terminating training\n",
      "Batch 157: Invalid loss, terminating training\n",
      "Batch 158: Invalid loss, terminating training\n",
      "Batch 159: Invalid loss, terminating training\n",
      "Batch 160: Invalid loss, terminating training\n",
      "Batch 161: Invalid loss, terminating training\n",
      "Batch 162: Invalid loss, terminating training\n",
      "Batch 163: Invalid loss, terminating training\n",
      "Batch 164: Invalid loss, terminating training\n",
      "Batch 165: Invalid loss, terminating training\n",
      "Batch 166: Invalid loss, terminating training\n",
      "Batch 167: Invalid loss, terminating training\n",
      "Batch 168: Invalid loss, terminating training\n",
      "Batch 169: Invalid loss, terminating training\n",
      "Batch 170: Invalid loss, terminating training\n",
      "Batch 171: Invalid loss, terminating training\n",
      "Batch 172: Invalid loss, terminating training\n",
      "Batch 173: Invalid loss, terminating training\n",
      "Batch 174: Invalid loss, terminating training\n",
      "Batch 175: Invalid loss, terminating training\n",
      "Batch 176: Invalid loss, terminating training\n",
      "Batch 177: Invalid loss, terminating training\n",
      "Batch 178: Invalid loss, terminating training\n",
      "Batch 179: Invalid loss, terminating training\n",
      "Batch 180: Invalid loss, terminating training\n",
      "Batch 181: Invalid loss, terminating training\n",
      "Batch 182: Invalid loss, terminating training\n",
      "Batch 183: Invalid loss, terminating training\n",
      "Batch 184: Invalid loss, terminating training\n",
      "Batch 185: Invalid loss, terminating training\n",
      "Batch 186: Invalid loss, terminating training\n",
      "Batch 187: Invalid loss, terminating training\n",
      "Batch 188: Invalid loss, terminating training\n",
      "Batch 189: Invalid loss, terminating training\n",
      "Batch 190: Invalid loss, terminating training\n",
      "Batch 191: Invalid loss, terminating training\n",
      "Batch 192: Invalid loss, terminating training\n",
      "Batch 193: Invalid loss, terminating training\n",
      "Batch 194: Invalid loss, terminating training\n",
      "Batch 195: Invalid loss, terminating training\n",
      "Batch 196: Invalid loss, terminating training\n",
      "Batch 197: Invalid loss, terminating training\n",
      "Batch 198: Invalid loss, terminating training\n",
      "Batch 199: Invalid loss, terminating training\n",
      "Batch 200: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 201/1981 [==>...........................] - ETA: 1s - loss: nan - rmsle: nanBatch 201: Invalid loss, terminating training\n",
      "Batch 202: Invalid loss, terminating training\n",
      "Batch 203: Invalid loss, terminating training\n",
      "Batch 204: Invalid loss, terminating training\n",
      "Batch 205: Invalid loss, terminating training\n",
      "Batch 206: Invalid loss, terminating training\n",
      "Batch 207: Invalid loss, terminating training\n",
      "Batch 208: Invalid loss, terminating training\n",
      "Batch 209: Invalid loss, terminating training\n",
      "Batch 210: Invalid loss, terminating training\n",
      "Batch 211: Invalid loss, terminating training\n",
      "Batch 212: Invalid loss, terminating training\n",
      "Batch 213: Invalid loss, terminating training\n",
      "Batch 214: Invalid loss, terminating training\n",
      "Batch 215: Invalid loss, terminating training\n",
      "Batch 216: Invalid loss, terminating training\n",
      "Batch 217: Invalid loss, terminating training\n",
      "Batch 218: Invalid loss, terminating training\n",
      "Batch 219: Invalid loss, terminating training\n",
      "Batch 220: Invalid loss, terminating training\n",
      "Batch 221: Invalid loss, terminating training\n",
      "Batch 222: Invalid loss, terminating training\n",
      "Batch 223: Invalid loss, terminating training\n",
      "Batch 224: Invalid loss, terminating training\n",
      "Batch 225: Invalid loss, terminating training\n",
      "Batch 226: Invalid loss, terminating training\n",
      "Batch 227: Invalid loss, terminating training\n",
      "Batch 228: Invalid loss, terminating training\n",
      "Batch 229: Invalid loss, terminating training\n",
      "Batch 230: Invalid loss, terminating training\n",
      "Batch 231: Invalid loss, terminating training\n",
      "Batch 232: Invalid loss, terminating training\n",
      "Batch 233: Invalid loss, terminating training\n",
      "Batch 234: Invalid loss, terminating training\n",
      "Batch 235: Invalid loss, terminating training\n",
      "Batch 236: Invalid loss, terminating training\n",
      "Batch 237: Invalid loss, terminating training\n",
      "Batch 238: Invalid loss, terminating training\n",
      "Batch 239: Invalid loss, terminating training\n",
      "Batch 240: Invalid loss, terminating training\n",
      "Batch 241: Invalid loss, terminating training\n",
      "Batch 242: Invalid loss, terminating training\n",
      "Batch 243: Invalid loss, terminating training\n",
      "Batch 244: Invalid loss, terminating training\n",
      "Batch 245: Invalid loss, terminating training\n",
      "Batch 246: Invalid loss, terminating training\n",
      "Batch 247: Invalid loss, terminating training\n",
      "Batch 248: Invalid loss, terminating training\n",
      "Batch 249: Invalid loss, terminating training\n",
      "Batch 250: Invalid loss, terminating training\n",
      "Batch 251: Invalid loss, terminating training\n",
      "Batch 252: Invalid loss, terminating training\n",
      "Batch 253: Invalid loss, terminating training\n",
      "Batch 254: Invalid loss, terminating training\n",
      "Batch 255: Invalid loss, terminating training\n",
      "Batch 256: Invalid loss, terminating training\n",
      "Batch 257: Invalid loss, terminating training\n",
      "Batch 258: Invalid loss, terminating training\n",
      "Batch 259: Invalid loss, terminating training\n",
      "Batch 260: Invalid loss, terminating training\n",
      "Batch 261: Invalid loss, terminating training\n",
      "Batch 262: Invalid loss, terminating training\n",
      "Batch 263: Invalid loss, terminating training\n",
      "Batch 264: Invalid loss, terminating training\n",
      "Batch 265: Invalid loss, terminating training\n",
      "Batch 266: Invalid loss, terminating training\n",
      "Batch 267: Invalid loss, terminating training\n",
      "Batch 268: Invalid loss, terminating training\n",
      "Batch 269: Invalid loss, terminating training\n",
      "Batch 270: Invalid loss, terminating training\n",
      "Batch 271: Invalid loss, terminating training\n",
      "Batch 272: Invalid loss, terminating training\n",
      "Batch 273: Invalid loss, terminating training\n",
      "Batch 274: Invalid loss, terminating training\n",
      "Batch 275: Invalid loss, terminating training\n",
      "Batch 276: Invalid loss, terminating training\n",
      "Batch 277: Invalid loss, terminating training\n",
      "Batch 278: Invalid loss, terminating training\n",
      "Batch 279: Invalid loss, terminating training\n",
      "Batch 280: Invalid loss, terminating training\n",
      "Batch 281: Invalid loss, terminating training\n",
      "Batch 282: Invalid loss, terminating training\n",
      "Batch 283: Invalid loss, terminating training\n",
      "Batch 284: Invalid loss, terminating training\n",
      "Batch 285: Invalid loss, terminating training\n",
      "Batch 286: Invalid loss, terminating training\n",
      " 287/1981 [===>..........................] - ETA: 1s - loss: nan - rmsle: nanBatch 287: Invalid loss, terminating training\n",
      "Batch 288: Invalid loss, terminating training\n",
      "Batch 289: Invalid loss, terminating training\n",
      "Batch 290: Invalid loss, terminating training\n",
      "Batch 291: Invalid loss, terminating training\n",
      "Batch 292: Invalid loss, terminating training\n",
      "Batch 293: Invalid loss, terminating training\n",
      "Batch 294: Invalid loss, terminating training\n",
      "Batch 295: Invalid loss, terminating training\n",
      "Batch 296: Invalid loss, terminating training\n",
      "Batch 297: Invalid loss, terminating training\n",
      "Batch 298: Invalid loss, terminating training\n",
      "Batch 299: Invalid loss, terminating training\n",
      "Batch 300: Invalid loss, terminating training\n",
      "Batch 301: Invalid loss, terminating training\n",
      "Batch 302: Invalid loss, terminating training\n",
      "Batch 303: Invalid loss, terminating training\n",
      "Batch 304: Invalid loss, terminating training\n",
      "Batch 305: Invalid loss, terminating training\n",
      "Batch 306: Invalid loss, terminating training\n",
      "Batch 307: Invalid loss, terminating training\n",
      "Batch 308: Invalid loss, terminating training\n",
      "Batch 309: Invalid loss, terminating training\n",
      "Batch 310: Invalid loss, terminating training\n",
      "Batch 311: Invalid loss, terminating training\n",
      "Batch 312: Invalid loss, terminating training\n",
      "Batch 313: Invalid loss, terminating training\n",
      "Batch 314: Invalid loss, terminating training\n",
      "Batch 315: Invalid loss, terminating training\n",
      "Batch 316: Invalid loss, terminating training\n",
      "Batch 317: Invalid loss, terminating training\n",
      "Batch 318: Invalid loss, terminating training\n",
      "Batch 319: Invalid loss, terminating training\n",
      "Batch 320: Invalid loss, terminating training\n",
      "Batch 321: Invalid loss, terminating training\n",
      "Batch 322: Invalid loss, terminating training\n",
      "Batch 323: Invalid loss, terminating training\n",
      "Batch 324: Invalid loss, terminating training\n",
      "Batch 325: Invalid loss, terminating training\n",
      "Batch 326: Invalid loss, terminating training\n",
      "Batch 327: Invalid loss, terminating training\n",
      "Batch 328: Invalid loss, terminating training\n",
      "Batch 329: Invalid loss, terminating training\n",
      "Batch 330: Invalid loss, terminating training\n",
      "Batch 331: Invalid loss, terminating training\n",
      "Batch 332: Invalid loss, terminating training\n",
      "Batch 333: Invalid loss, terminating training\n",
      "Batch 334: Invalid loss, terminating training\n",
      "Batch 335: Invalid loss, terminating training\n",
      "Batch 336: Invalid loss, terminating training\n",
      "Batch 337: Invalid loss, terminating training\n",
      "Batch 338: Invalid loss, terminating training\n",
      "Batch 339: Invalid loss, terminating training\n",
      "Batch 340: Invalid loss, terminating training\n",
      "Batch 341: Invalid loss, terminating training\n",
      "Batch 342: Invalid loss, terminating training\n",
      "Batch 343: Invalid loss, terminating training\n",
      "Batch 344: Invalid loss, terminating training\n",
      "Batch 345: Invalid loss, terminating training\n",
      "Batch 346: Invalid loss, terminating training\n",
      "Batch 347: Invalid loss, terminating training\n",
      "Batch 348: Invalid loss, terminating training\n",
      "Batch 349: Invalid loss, terminating training\n",
      "Batch 350: Invalid loss, terminating training\n",
      "Batch 351: Invalid loss, terminating training\n",
      "Batch 352: Invalid loss, terminating training\n",
      "Batch 353: Invalid loss, terminating training\n",
      "Batch 354: Invalid loss, terminating training\n",
      "Batch 355: Invalid loss, terminating training\n",
      "Batch 356: Invalid loss, terminating training\n",
      "Batch 357: Invalid loss, terminating training\n",
      "Batch 358: Invalid loss, terminating training\n",
      "Batch 359: Invalid loss, terminating training\n",
      "Batch 360: Invalid loss, terminating training\n",
      "Batch 361: Invalid loss, terminating training\n",
      "Batch 362: Invalid loss, terminating training\n",
      "Batch 363: Invalid loss, terminating training\n",
      "Batch 364: Invalid loss, terminating training\n",
      "Batch 365: Invalid loss, terminating training\n",
      "Batch 366: Invalid loss, terminating training\n",
      "Batch 367: Invalid loss, terminating training\n",
      "Batch 368: Invalid loss, terminating training\n",
      "Batch 369: Invalid loss, terminating training\n",
      "Batch 370: Invalid loss, terminating training\n",
      "Batch 371: Invalid loss, terminating training\n",
      "Batch 372: Invalid loss, terminating training\n",
      "Batch 373: Invalid loss, terminating training\n",
      "Batch 374: Invalid loss, terminating training\n",
      "Batch 375: Invalid loss, terminating training\n",
      "Batch 376: Invalid loss, terminating training\n",
      "Batch 377: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 378/1981 [====>.........................] - ETA: 1s - loss: nan - rmsle: nanBatch 378: Invalid loss, terminating training\n",
      "Batch 379: Invalid loss, terminating training\n",
      "Batch 380: Invalid loss, terminating training\n",
      "Batch 381: Invalid loss, terminating training\n",
      "Batch 382: Invalid loss, terminating training\n",
      "Batch 383: Invalid loss, terminating training\n",
      "Batch 384: Invalid loss, terminating training\n",
      "Batch 385: Invalid loss, terminating training\n",
      "Batch 386: Invalid loss, terminating training\n",
      "Batch 387: Invalid loss, terminating training\n",
      "Batch 388: Invalid loss, terminating training\n",
      "Batch 389: Invalid loss, terminating training\n",
      "Batch 390: Invalid loss, terminating training\n",
      "Batch 391: Invalid loss, terminating training\n",
      "Batch 392: Invalid loss, terminating training\n",
      "Batch 393: Invalid loss, terminating training\n",
      "Batch 394: Invalid loss, terminating training\n",
      "Batch 395: Invalid loss, terminating training\n",
      "Batch 396: Invalid loss, terminating training\n",
      "Batch 397: Invalid loss, terminating training\n",
      "Batch 398: Invalid loss, terminating training\n",
      "Batch 399: Invalid loss, terminating training\n",
      "Batch 400: Invalid loss, terminating training\n",
      "Batch 401: Invalid loss, terminating training\n",
      "Batch 402: Invalid loss, terminating training\n",
      "Batch 403: Invalid loss, terminating training\n",
      "Batch 404: Invalid loss, terminating training\n",
      "Batch 405: Invalid loss, terminating training\n",
      "Batch 406: Invalid loss, terminating training\n",
      "Batch 407: Invalid loss, terminating training\n",
      "Batch 408: Invalid loss, terminating training\n",
      "Batch 409: Invalid loss, terminating training\n",
      "Batch 410: Invalid loss, terminating training\n",
      "Batch 411: Invalid loss, terminating training\n",
      "Batch 412: Invalid loss, terminating training\n",
      "Batch 413: Invalid loss, terminating training\n",
      "Batch 414: Invalid loss, terminating training\n",
      "Batch 415: Invalid loss, terminating training\n",
      "Batch 416: Invalid loss, terminating training\n",
      "Batch 417: Invalid loss, terminating training\n",
      "Batch 418: Invalid loss, terminating training\n",
      "Batch 419: Invalid loss, terminating training\n",
      "Batch 420: Invalid loss, terminating training\n",
      "Batch 421: Invalid loss, terminating training\n",
      "Batch 422: Invalid loss, terminating training\n",
      "Batch 423: Invalid loss, terminating training\n",
      "Batch 424: Invalid loss, terminating training\n",
      "Batch 425: Invalid loss, terminating training\n",
      "Batch 426: Invalid loss, terminating training\n",
      "Batch 427: Invalid loss, terminating training\n",
      "Batch 428: Invalid loss, terminating training\n",
      "Batch 429: Invalid loss, terminating training\n",
      "Batch 430: Invalid loss, terminating training\n",
      "Batch 431: Invalid loss, terminating training\n",
      "Batch 432: Invalid loss, terminating training\n",
      "Batch 433: Invalid loss, terminating training\n",
      "Batch 434: Invalid loss, terminating training\n",
      "Batch 435: Invalid loss, terminating training\n",
      "Batch 436: Invalid loss, terminating training\n",
      "Batch 437: Invalid loss, terminating training\n",
      "Batch 438: Invalid loss, terminating training\n",
      "Batch 439: Invalid loss, terminating training\n",
      "Batch 440: Invalid loss, terminating training\n",
      "Batch 441: Invalid loss, terminating training\n",
      "Batch 442: Invalid loss, terminating training\n",
      "Batch 443: Invalid loss, terminating training\n",
      "Batch 444: Invalid loss, terminating training\n",
      "Batch 445: Invalid loss, terminating training\n",
      "Batch 446: Invalid loss, terminating training\n",
      "Batch 447: Invalid loss, terminating training\n",
      "Batch 448: Invalid loss, terminating training\n",
      "Batch 449: Invalid loss, terminating training\n",
      "Batch 450: Invalid loss, terminating training\n",
      "Batch 451: Invalid loss, terminating training\n",
      "Batch 452: Invalid loss, terminating training\n",
      "Batch 453: Invalid loss, terminating training\n",
      "Batch 454: Invalid loss, terminating training\n",
      "Batch 455: Invalid loss, terminating training\n",
      "Batch 456: Invalid loss, terminating training\n",
      "Batch 457: Invalid loss, terminating training\n",
      "Batch 458: Invalid loss, terminating training\n",
      "Batch 459: Invalid loss, terminating training\n",
      "Batch 460: Invalid loss, terminating training\n",
      "Batch 461: Invalid loss, terminating training\n",
      "Batch 462: Invalid loss, terminating training\n",
      "Batch 463: Invalid loss, terminating training\n",
      "Batch 464: Invalid loss, terminating training\n",
      "Batch 465: Invalid loss, terminating training\n",
      "Batch 466: Invalid loss, terminating training\n",
      "Batch 467: Invalid loss, terminating training\n",
      "Batch 468: Invalid loss, terminating training\n",
      "Batch 469: Invalid loss, terminating training\n",
      " 470/1981 [======>.......................] - ETA: 0s - loss: nan - rmsle: nanBatch 470: Invalid loss, terminating training\n",
      "Batch 471: Invalid loss, terminating training\n",
      "Batch 472: Invalid loss, terminating training\n",
      "Batch 473: Invalid loss, terminating training\n",
      "Batch 474: Invalid loss, terminating training\n",
      "Batch 475: Invalid loss, terminating training\n",
      "Batch 476: Invalid loss, terminating training\n",
      "Batch 477: Invalid loss, terminating training\n",
      "Batch 478: Invalid loss, terminating training\n",
      "Batch 479: Invalid loss, terminating training\n",
      "Batch 480: Invalid loss, terminating training\n",
      "Batch 481: Invalid loss, terminating training\n",
      "Batch 482: Invalid loss, terminating training\n",
      "Batch 483: Invalid loss, terminating training\n",
      "Batch 484: Invalid loss, terminating training\n",
      "Batch 485: Invalid loss, terminating training\n",
      "Batch 486: Invalid loss, terminating training\n",
      "Batch 487: Invalid loss, terminating training\n",
      "Batch 488: Invalid loss, terminating training\n",
      "Batch 489: Invalid loss, terminating training\n",
      "Batch 490: Invalid loss, terminating training\n",
      "Batch 491: Invalid loss, terminating training\n",
      "Batch 492: Invalid loss, terminating training\n",
      "Batch 493: Invalid loss, terminating training\n",
      "Batch 494: Invalid loss, terminating training\n",
      "Batch 495: Invalid loss, terminating training\n",
      "Batch 496: Invalid loss, terminating training\n",
      "Batch 497: Invalid loss, terminating training\n",
      "Batch 498: Invalid loss, terminating training\n",
      "Batch 499: Invalid loss, terminating training\n",
      "Batch 500: Invalid loss, terminating training\n",
      "Batch 501: Invalid loss, terminating training\n",
      "Batch 502: Invalid loss, terminating training\n",
      "Batch 503: Invalid loss, terminating training\n",
      "Batch 504: Invalid loss, terminating training\n",
      "Batch 505: Invalid loss, terminating training\n",
      "Batch 506: Invalid loss, terminating training\n",
      "Batch 507: Invalid loss, terminating training\n",
      "Batch 508: Invalid loss, terminating training\n",
      "Batch 509: Invalid loss, terminating training\n",
      "Batch 510: Invalid loss, terminating training\n",
      "Batch 511: Invalid loss, terminating training\n",
      "Batch 512: Invalid loss, terminating training\n",
      "Batch 513: Invalid loss, terminating training\n",
      "Batch 514: Invalid loss, terminating training\n",
      "Batch 515: Invalid loss, terminating training\n",
      "Batch 516: Invalid loss, terminating training\n",
      "Batch 517: Invalid loss, terminating training\n",
      "Batch 518: Invalid loss, terminating training\n",
      "Batch 519: Invalid loss, terminating training\n",
      "Batch 520: Invalid loss, terminating training\n",
      "Batch 521: Invalid loss, terminating training\n",
      "Batch 522: Invalid loss, terminating training\n",
      "Batch 523: Invalid loss, terminating training\n",
      "Batch 524: Invalid loss, terminating training\n",
      "Batch 525: Invalid loss, terminating training\n",
      "Batch 526: Invalid loss, terminating training\n",
      "Batch 527: Invalid loss, terminating training\n",
      "Batch 528: Invalid loss, terminating training\n",
      "Batch 529: Invalid loss, terminating training\n",
      "Batch 530: Invalid loss, terminating training\n",
      "Batch 531: Invalid loss, terminating training\n",
      "Batch 532: Invalid loss, terminating training\n",
      "Batch 533: Invalid loss, terminating training\n",
      "Batch 534: Invalid loss, terminating training\n",
      "Batch 535: Invalid loss, terminating training\n",
      "Batch 536: Invalid loss, terminating training\n",
      "Batch 537: Invalid loss, terminating training\n",
      "Batch 538: Invalid loss, terminating training\n",
      "Batch 539: Invalid loss, terminating training\n",
      "Batch 540: Invalid loss, terminating training\n",
      "Batch 541: Invalid loss, terminating training\n",
      "Batch 542: Invalid loss, terminating training\n",
      "Batch 543: Invalid loss, terminating training\n",
      "Batch 544: Invalid loss, terminating training\n",
      "Batch 545: Invalid loss, terminating training\n",
      "Batch 546: Invalid loss, terminating training\n",
      "Batch 547: Invalid loss, terminating training\n",
      "Batch 548: Invalid loss, terminating training\n",
      "Batch 549: Invalid loss, terminating training\n",
      "Batch 550: Invalid loss, terminating training\n",
      "Batch 551: Invalid loss, terminating training\n",
      "Batch 552: Invalid loss, terminating training\n",
      "Batch 553: Invalid loss, terminating training\n",
      "Batch 554: Invalid loss, terminating training\n",
      "Batch 555: Invalid loss, terminating training\n",
      "Batch 556: Invalid loss, terminating training\n",
      "Batch 557: Invalid loss, terminating training\n",
      "Batch 558: Invalid loss, terminating training\n",
      "Batch 559: Invalid loss, terminating training\n",
      "Batch 560: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 561: Invalid loss, terminating training\n",
      " 562/1981 [=======>......................] - ETA: 0s - loss: nan - rmsle: nanBatch 562: Invalid loss, terminating training\n",
      "Batch 563: Invalid loss, terminating training\n",
      "Batch 564: Invalid loss, terminating training\n",
      "Batch 565: Invalid loss, terminating training\n",
      "Batch 566: Invalid loss, terminating training\n",
      "Batch 567: Invalid loss, terminating training\n",
      "Batch 568: Invalid loss, terminating training\n",
      "Batch 569: Invalid loss, terminating training\n",
      "Batch 570: Invalid loss, terminating training\n",
      "Batch 571: Invalid loss, terminating training\n",
      "Batch 572: Invalid loss, terminating training\n",
      "Batch 573: Invalid loss, terminating training\n",
      "Batch 574: Invalid loss, terminating training\n",
      "Batch 575: Invalid loss, terminating training\n",
      "Batch 576: Invalid loss, terminating training\n",
      "Batch 577: Invalid loss, terminating training\n",
      "Batch 578: Invalid loss, terminating training\n",
      "Batch 579: Invalid loss, terminating training\n",
      "Batch 580: Invalid loss, terminating training\n",
      "Batch 581: Invalid loss, terminating training\n",
      "Batch 582: Invalid loss, terminating training\n",
      "Batch 583: Invalid loss, terminating training\n",
      "Batch 584: Invalid loss, terminating training\n",
      "Batch 585: Invalid loss, terminating training\n",
      "Batch 586: Invalid loss, terminating training\n",
      "Batch 587: Invalid loss, terminating training\n",
      "Batch 588: Invalid loss, terminating training\n",
      "Batch 589: Invalid loss, terminating training\n",
      "Batch 590: Invalid loss, terminating training\n",
      "Batch 591: Invalid loss, terminating training\n",
      "Batch 592: Invalid loss, terminating training\n",
      "Batch 593: Invalid loss, terminating training\n",
      "Batch 594: Invalid loss, terminating training\n",
      "Batch 595: Invalid loss, terminating training\n",
      "Batch 596: Invalid loss, terminating training\n",
      "Batch 597: Invalid loss, terminating training\n",
      "Batch 598: Invalid loss, terminating training\n",
      "Batch 599: Invalid loss, terminating training\n",
      "Batch 600: Invalid loss, terminating training\n",
      "Batch 601: Invalid loss, terminating training\n",
      "Batch 602: Invalid loss, terminating training\n",
      "Batch 603: Invalid loss, terminating training\n",
      "Batch 604: Invalid loss, terminating training\n",
      "Batch 605: Invalid loss, terminating training\n",
      "Batch 606: Invalid loss, terminating training\n",
      "Batch 607: Invalid loss, terminating training\n",
      "Batch 608: Invalid loss, terminating training\n",
      "Batch 609: Invalid loss, terminating training\n",
      "Batch 610: Invalid loss, terminating training\n",
      "Batch 611: Invalid loss, terminating training\n",
      "Batch 612: Invalid loss, terminating training\n",
      "Batch 613: Invalid loss, terminating training\n",
      "Batch 614: Invalid loss, terminating training\n",
      "Batch 615: Invalid loss, terminating training\n",
      "Batch 616: Invalid loss, terminating training\n",
      "Batch 617: Invalid loss, terminating training\n",
      "Batch 618: Invalid loss, terminating training\n",
      "Batch 619: Invalid loss, terminating training\n",
      "Batch 620: Invalid loss, terminating training\n",
      "Batch 621: Invalid loss, terminating training\n",
      "Batch 622: Invalid loss, terminating training\n",
      "Batch 623: Invalid loss, terminating training\n",
      "Batch 624: Invalid loss, terminating training\n",
      "Batch 625: Invalid loss, terminating training\n",
      "Batch 626: Invalid loss, terminating training\n",
      "Batch 627: Invalid loss, terminating training\n",
      "Batch 628: Invalid loss, terminating training\n",
      "Batch 629: Invalid loss, terminating training\n",
      "Batch 630: Invalid loss, terminating training\n",
      "Batch 631: Invalid loss, terminating training\n",
      "Batch 632: Invalid loss, terminating training\n",
      "Batch 633: Invalid loss, terminating training\n",
      "Batch 634: Invalid loss, terminating training\n",
      "Batch 635: Invalid loss, terminating training\n",
      "Batch 636: Invalid loss, terminating training\n",
      "Batch 637: Invalid loss, terminating training\n",
      "Batch 638: Invalid loss, terminating training\n",
      "Batch 639: Invalid loss, terminating training\n",
      "Batch 640: Invalid loss, terminating training\n",
      "Batch 641: Invalid loss, terminating training\n",
      "Batch 642: Invalid loss, terminating training\n",
      "Batch 643: Invalid loss, terminating training\n",
      "Batch 644: Invalid loss, terminating training\n",
      "Batch 645: Invalid loss, terminating training\n",
      "Batch 646: Invalid loss, terminating training\n",
      "Batch 647: Invalid loss, terminating training\n",
      "Batch 648: Invalid loss, terminating training\n",
      "Batch 649: Invalid loss, terminating training\n",
      "Batch 650: Invalid loss, terminating training\n",
      "Batch 651: Invalid loss, terminating training\n",
      " 652/1981 [========>.....................] - ETA: 0s - loss: nan - rmsle: nanBatch 652: Invalid loss, terminating training\n",
      "Batch 653: Invalid loss, terminating training\n",
      "Batch 654: Invalid loss, terminating training\n",
      "Batch 655: Invalid loss, terminating training\n",
      "Batch 656: Invalid loss, terminating training\n",
      "Batch 657: Invalid loss, terminating training\n",
      "Batch 658: Invalid loss, terminating training\n",
      "Batch 659: Invalid loss, terminating training\n",
      "Batch 660: Invalid loss, terminating training\n",
      "Batch 661: Invalid loss, terminating training\n",
      "Batch 662: Invalid loss, terminating training\n",
      "Batch 663: Invalid loss, terminating training\n",
      "Batch 664: Invalid loss, terminating training\n",
      "Batch 665: Invalid loss, terminating training\n",
      "Batch 666: Invalid loss, terminating training\n",
      "Batch 667: Invalid loss, terminating training\n",
      "Batch 668: Invalid loss, terminating training\n",
      "Batch 669: Invalid loss, terminating training\n",
      "Batch 670: Invalid loss, terminating training\n",
      "Batch 671: Invalid loss, terminating training\n",
      "Batch 672: Invalid loss, terminating training\n",
      "Batch 673: Invalid loss, terminating training\n",
      "Batch 674: Invalid loss, terminating training\n",
      "Batch 675: Invalid loss, terminating training\n",
      "Batch 676: Invalid loss, terminating training\n",
      "Batch 677: Invalid loss, terminating training\n",
      "Batch 678: Invalid loss, terminating training\n",
      "Batch 679: Invalid loss, terminating training\n",
      "Batch 680: Invalid loss, terminating training\n",
      "Batch 681: Invalid loss, terminating training\n",
      "Batch 682: Invalid loss, terminating training\n",
      "Batch 683: Invalid loss, terminating training\n",
      "Batch 684: Invalid loss, terminating training\n",
      "Batch 685: Invalid loss, terminating training\n",
      "Batch 686: Invalid loss, terminating training\n",
      "Batch 687: Invalid loss, terminating training\n",
      "Batch 688: Invalid loss, terminating training\n",
      "Batch 689: Invalid loss, terminating training\n",
      "Batch 690: Invalid loss, terminating training\n",
      "Batch 691: Invalid loss, terminating training\n",
      "Batch 692: Invalid loss, terminating training\n",
      "Batch 693: Invalid loss, terminating training\n",
      "Batch 694: Invalid loss, terminating training\n",
      "Batch 695: Invalid loss, terminating training\n",
      "Batch 696: Invalid loss, terminating training\n",
      "Batch 697: Invalid loss, terminating training\n",
      "Batch 698: Invalid loss, terminating training\n",
      "Batch 699: Invalid loss, terminating training\n",
      "Batch 700: Invalid loss, terminating training\n",
      "Batch 701: Invalid loss, terminating training\n",
      "Batch 702: Invalid loss, terminating training\n",
      "Batch 703: Invalid loss, terminating training\n",
      "Batch 704: Invalid loss, terminating training\n",
      "Batch 705: Invalid loss, terminating training\n",
      "Batch 706: Invalid loss, terminating training\n",
      "Batch 707: Invalid loss, terminating training\n",
      "Batch 708: Invalid loss, terminating training\n",
      "Batch 709: Invalid loss, terminating training\n",
      "Batch 710: Invalid loss, terminating training\n",
      "Batch 711: Invalid loss, terminating training\n",
      "Batch 712: Invalid loss, terminating training\n",
      "Batch 713: Invalid loss, terminating training\n",
      "Batch 714: Invalid loss, terminating training\n",
      "Batch 715: Invalid loss, terminating training\n",
      "Batch 716: Invalid loss, terminating training\n",
      "Batch 717: Invalid loss, terminating training\n",
      "Batch 718: Invalid loss, terminating training\n",
      "Batch 719: Invalid loss, terminating training\n",
      " 720/1981 [=========>....................] - ETA: 0s - loss: nan - rmsle: nanBatch 720: Invalid loss, terminating training\n",
      "Batch 721: Invalid loss, terminating training\n",
      "Batch 722: Invalid loss, terminating training\n",
      "Batch 723: Invalid loss, terminating training\n",
      "Batch 724: Invalid loss, terminating training\n",
      "Batch 725: Invalid loss, terminating training\n",
      "Batch 726: Invalid loss, terminating training\n",
      "Batch 727: Invalid loss, terminating training\n",
      "Batch 728: Invalid loss, terminating training\n",
      "Batch 729: Invalid loss, terminating training\n",
      "Batch 730: Invalid loss, terminating training\n",
      "Batch 731: Invalid loss, terminating training\n",
      "Batch 732: Invalid loss, terminating training\n",
      "Batch 733: Invalid loss, terminating training\n",
      "Batch 734: Invalid loss, terminating training\n",
      "Batch 735: Invalid loss, terminating training\n",
      "Batch 736: Invalid loss, terminating training\n",
      "Batch 737: Invalid loss, terminating training\n",
      "Batch 738: Invalid loss, terminating training\n",
      "Batch 739: Invalid loss, terminating training\n",
      "Batch 740: Invalid loss, terminating training\n",
      "Batch 741: Invalid loss, terminating training\n",
      "Batch 742: Invalid loss, terminating training\n",
      "Batch 743: Invalid loss, terminating training\n",
      "Batch 744: Invalid loss, terminating training\n",
      "Batch 745: Invalid loss, terminating training\n",
      "Batch 746: Invalid loss, terminating training\n",
      "Batch 747: Invalid loss, terminating training\n",
      "Batch 748: Invalid loss, terminating training\n",
      "Batch 749: Invalid loss, terminating training\n",
      "Batch 750: Invalid loss, terminating training\n",
      "Batch 751: Invalid loss, terminating training\n",
      "Batch 752: Invalid loss, terminating training\n",
      "Batch 753: Invalid loss, terminating training\n",
      "Batch 754: Invalid loss, terminating training\n",
      "Batch 755: Invalid loss, terminating training\n",
      "Batch 756: Invalid loss, terminating training\n",
      "Batch 757: Invalid loss, terminating training\n",
      "Batch 758: Invalid loss, terminating training\n",
      "Batch 759: Invalid loss, terminating training\n",
      "Batch 760: Invalid loss, terminating training\n",
      "Batch 761: Invalid loss, terminating training\n",
      "Batch 762: Invalid loss, terminating training\n",
      "Batch 763: Invalid loss, terminating training\n",
      "Batch 764: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 765/1981 [==========>...................] - ETA: 0s - loss: nan - rmsle: nanBatch 765: Invalid loss, terminating training\n",
      "Batch 766: Invalid loss, terminating training\n",
      "Batch 767: Invalid loss, terminating training\n",
      "Batch 768: Invalid loss, terminating training\n",
      "Batch 769: Invalid loss, terminating training\n",
      "Batch 770: Invalid loss, terminating training\n",
      "Batch 771: Invalid loss, terminating training\n",
      "Batch 772: Invalid loss, terminating training\n",
      "Batch 773: Invalid loss, terminating training\n",
      "Batch 774: Invalid loss, terminating training\n",
      "Batch 775: Invalid loss, terminating training\n",
      "Batch 776: Invalid loss, terminating training\n",
      "Batch 777: Invalid loss, terminating training\n",
      "Batch 778: Invalid loss, terminating training\n",
      "Batch 779: Invalid loss, terminating training\n",
      "Batch 780: Invalid loss, terminating training\n",
      "Batch 781: Invalid loss, terminating training\n",
      "Batch 782: Invalid loss, terminating training\n",
      "Batch 783: Invalid loss, terminating training\n",
      "Batch 784: Invalid loss, terminating training\n",
      "Batch 785: Invalid loss, terminating training\n",
      "Batch 786: Invalid loss, terminating training\n",
      "Batch 787: Invalid loss, terminating training\n",
      "Batch 788: Invalid loss, terminating training\n",
      "Batch 789: Invalid loss, terminating training\n",
      "Batch 790: Invalid loss, terminating training\n",
      "Batch 791: Invalid loss, terminating training\n",
      "Batch 792: Invalid loss, terminating training\n",
      "Batch 793: Invalid loss, terminating training\n",
      "Batch 794: Invalid loss, terminating training\n",
      "Batch 795: Invalid loss, terminating training\n",
      "Batch 796: Invalid loss, terminating training\n",
      "Batch 797: Invalid loss, terminating training\n",
      "Batch 798: Invalid loss, terminating training\n",
      "Batch 799: Invalid loss, terminating training\n",
      "Batch 800: Invalid loss, terminating training\n",
      "Batch 801: Invalid loss, terminating training\n",
      "Batch 802: Invalid loss, terminating training\n",
      "Batch 803: Invalid loss, terminating training\n",
      "Batch 804: Invalid loss, terminating training\n",
      "Batch 805: Invalid loss, terminating training\n",
      "Batch 806: Invalid loss, terminating training\n",
      "Batch 807: Invalid loss, terminating training\n",
      "Batch 808: Invalid loss, terminating training\n",
      "Batch 809: Invalid loss, terminating training\n",
      "Batch 810: Invalid loss, terminating training\n",
      "Batch 811: Invalid loss, terminating training\n",
      "Batch 812: Invalid loss, terminating training\n",
      "Batch 813: Invalid loss, terminating training\n",
      "Batch 814: Invalid loss, terminating training\n",
      "Batch 815: Invalid loss, terminating training\n",
      "Batch 816: Invalid loss, terminating training\n",
      "Batch 817: Invalid loss, terminating training\n",
      "Batch 818: Invalid loss, terminating training\n",
      "Batch 819: Invalid loss, terminating training\n",
      "Batch 820: Invalid loss, terminating training\n",
      "Batch 821: Invalid loss, terminating training\n",
      "Batch 822: Invalid loss, terminating training\n",
      " 823/1981 [===========>..................] - ETA: 0s - loss: nan - rmsle: nanBatch 823: Invalid loss, terminating training\n",
      "Batch 824: Invalid loss, terminating training\n",
      "Batch 825: Invalid loss, terminating training\n",
      "Batch 826: Invalid loss, terminating training\n",
      "Batch 827: Invalid loss, terminating training\n",
      "Batch 828: Invalid loss, terminating training\n",
      "Batch 829: Invalid loss, terminating training\n",
      "Batch 830: Invalid loss, terminating training\n",
      "Batch 831: Invalid loss, terminating training\n",
      "Batch 832: Invalid loss, terminating training\n",
      "Batch 833: Invalid loss, terminating training\n",
      "Batch 834: Invalid loss, terminating training\n",
      "Batch 835: Invalid loss, terminating training\n",
      "Batch 836: Invalid loss, terminating training\n",
      "Batch 837: Invalid loss, terminating training\n",
      "Batch 838: Invalid loss, terminating training\n",
      "Batch 839: Invalid loss, terminating training\n",
      "Batch 840: Invalid loss, terminating training\n",
      "Batch 841: Invalid loss, terminating training\n",
      "Batch 842: Invalid loss, terminating training\n",
      "Batch 843: Invalid loss, terminating training\n",
      "Batch 844: Invalid loss, terminating training\n",
      "Batch 845: Invalid loss, terminating training\n",
      "Batch 846: Invalid loss, terminating training\n",
      "Batch 847: Invalid loss, terminating training\n",
      "Batch 848: Invalid loss, terminating training\n",
      "Batch 849: Invalid loss, terminating training\n",
      "Batch 850: Invalid loss, terminating training\n",
      "Batch 851: Invalid loss, terminating training\n",
      "Batch 852: Invalid loss, terminating training\n",
      "Batch 853: Invalid loss, terminating training\n",
      "Batch 854: Invalid loss, terminating training\n",
      "Batch 855: Invalid loss, terminating training\n",
      "Batch 856: Invalid loss, terminating training\n",
      "Batch 857: Invalid loss, terminating training\n",
      "Batch 858: Invalid loss, terminating training\n",
      "Batch 859: Invalid loss, terminating training\n",
      "Batch 860: Invalid loss, terminating training\n",
      "Batch 861: Invalid loss, terminating training\n",
      "Batch 862: Invalid loss, terminating training\n",
      "Batch 863: Invalid loss, terminating training\n",
      "Batch 864: Invalid loss, terminating training\n",
      "Batch 865: Invalid loss, terminating training\n",
      "Batch 866: Invalid loss, terminating training\n",
      "Batch 867: Invalid loss, terminating training\n",
      "Batch 868: Invalid loss, terminating training\n",
      "Batch 869: Invalid loss, terminating training\n",
      "Batch 870: Invalid loss, terminating training\n",
      "Batch 871: Invalid loss, terminating training\n",
      " 872/1981 [============>.................] - ETA: 0s - loss: nan - rmsle: nanBatch 872: Invalid loss, terminating training\n",
      "Batch 873: Invalid loss, terminating training\n",
      "Batch 874: Invalid loss, terminating training\n",
      "Batch 875: Invalid loss, terminating training\n",
      "Batch 876: Invalid loss, terminating training\n",
      "Batch 877: Invalid loss, terminating training\n",
      "Batch 878: Invalid loss, terminating training\n",
      "Batch 879: Invalid loss, terminating training\n",
      "Batch 880: Invalid loss, terminating training\n",
      "Batch 881: Invalid loss, terminating training\n",
      "Batch 882: Invalid loss, terminating training\n",
      "Batch 883: Invalid loss, terminating training\n",
      "Batch 884: Invalid loss, terminating training\n",
      "Batch 885: Invalid loss, terminating training\n",
      "Batch 886: Invalid loss, terminating training\n",
      "Batch 887: Invalid loss, terminating training\n",
      "Batch 888: Invalid loss, terminating training\n",
      "Batch 889: Invalid loss, terminating training\n",
      "Batch 890: Invalid loss, terminating training\n",
      "Batch 891: Invalid loss, terminating training\n",
      "Batch 892: Invalid loss, terminating training\n",
      "Batch 893: Invalid loss, terminating training\n",
      "Batch 894: Invalid loss, terminating training\n",
      "Batch 895: Invalid loss, terminating training\n",
      "Batch 896: Invalid loss, terminating training\n",
      "Batch 897: Invalid loss, terminating training\n",
      "Batch 898: Invalid loss, terminating training\n",
      "Batch 899: Invalid loss, terminating training\n",
      "Batch 900: Invalid loss, terminating training\n",
      "Batch 901: Invalid loss, terminating training\n",
      "Batch 902: Invalid loss, terminating training\n",
      "Batch 903: Invalid loss, terminating training\n",
      "Batch 904: Invalid loss, terminating training\n",
      "Batch 905: Invalid loss, terminating training\n",
      "Batch 906: Invalid loss, terminating training\n",
      "Batch 907: Invalid loss, terminating training\n",
      "Batch 908: Invalid loss, terminating training\n",
      "Batch 909: Invalid loss, terminating training\n",
      " 910/1981 [============>.................] - ETA: 0s - loss: nan - rmsle: nanBatch 910: Invalid loss, terminating training\n",
      "Batch 911: Invalid loss, terminating training\n",
      "Batch 912: Invalid loss, terminating training\n",
      "Batch 913: Invalid loss, terminating training\n",
      "Batch 914: Invalid loss, terminating training\n",
      "Batch 915: Invalid loss, terminating training\n",
      "Batch 916: Invalid loss, terminating training\n",
      "Batch 917: Invalid loss, terminating training\n",
      "Batch 918: Invalid loss, terminating training\n",
      "Batch 919: Invalid loss, terminating training\n",
      "Batch 920: Invalid loss, terminating training\n",
      "Batch 921: Invalid loss, terminating training\n",
      "Batch 922: Invalid loss, terminating training\n",
      "Batch 923: Invalid loss, terminating training\n",
      "Batch 924: Invalid loss, terminating training\n",
      "Batch 925: Invalid loss, terminating training\n",
      "Batch 926: Invalid loss, terminating training\n",
      "Batch 927: Invalid loss, terminating training\n",
      "Batch 928: Invalid loss, terminating training\n",
      "Batch 929: Invalid loss, terminating training\n",
      "Batch 930: Invalid loss, terminating training\n",
      "Batch 931: Invalid loss, terminating training\n",
      "Batch 932: Invalid loss, terminating training\n",
      "Batch 933: Invalid loss, terminating training\n",
      "Batch 934: Invalid loss, terminating training\n",
      "Batch 935: Invalid loss, terminating training\n",
      "Batch 936: Invalid loss, terminating training\n",
      "Batch 937: Invalid loss, terminating training\n",
      "Batch 938: Invalid loss, terminating training\n",
      "Batch 939: Invalid loss, terminating training\n",
      "Batch 940: Invalid loss, terminating training\n",
      "Batch 941: Invalid loss, terminating training\n",
      "Batch 942: Invalid loss, terminating training\n",
      "Batch 943: Invalid loss, terminating training\n",
      "Batch 944: Invalid loss, terminating training\n",
      "Batch 945: Invalid loss, terminating training\n",
      "Batch 946: Invalid loss, terminating training\n",
      "Batch 947: Invalid loss, terminating training\n",
      "Batch 948: Invalid loss, terminating training\n",
      "Batch 949: Invalid loss, terminating training\n",
      "Batch 950: Invalid loss, terminating training\n",
      "Batch 951: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 952/1981 [=============>................] - ETA: 0s - loss: nan - rmsle: nanBatch 952: Invalid loss, terminating training\n",
      "Batch 953: Invalid loss, terminating training\n",
      "Batch 954: Invalid loss, terminating training\n",
      "Batch 955: Invalid loss, terminating training\n",
      "Batch 956: Invalid loss, terminating training\n",
      "Batch 957: Invalid loss, terminating training\n",
      "Batch 958: Invalid loss, terminating training\n",
      "Batch 959: Invalid loss, terminating training\n",
      "Batch 960: Invalid loss, terminating training\n",
      "Batch 961: Invalid loss, terminating training\n",
      "Batch 962: Invalid loss, terminating training\n",
      "Batch 963: Invalid loss, terminating training\n",
      "Batch 964: Invalid loss, terminating training\n",
      "Batch 965: Invalid loss, terminating training\n",
      "Batch 966: Invalid loss, terminating training\n",
      "Batch 967: Invalid loss, terminating training\n",
      "Batch 968: Invalid loss, terminating training\n",
      "Batch 969: Invalid loss, terminating training\n",
      "Batch 970: Invalid loss, terminating training\n",
      "Batch 971: Invalid loss, terminating training\n",
      "Batch 972: Invalid loss, terminating training\n",
      "Batch 973: Invalid loss, terminating training\n",
      "Batch 974: Invalid loss, terminating training\n",
      "Batch 975: Invalid loss, terminating training\n",
      "Batch 976: Invalid loss, terminating training\n",
      "Batch 977: Invalid loss, terminating training\n",
      "Batch 978: Invalid loss, terminating training\n",
      "Batch 979: Invalid loss, terminating training\n",
      "Batch 980: Invalid loss, terminating training\n",
      "Batch 981: Invalid loss, terminating training\n",
      "Batch 982: Invalid loss, terminating training\n",
      "Batch 983: Invalid loss, terminating training\n",
      "Batch 984: Invalid loss, terminating training\n",
      "Batch 985: Invalid loss, terminating training\n",
      "Batch 986: Invalid loss, terminating training\n",
      "Batch 987: Invalid loss, terminating training\n",
      "Batch 988: Invalid loss, terminating training\n",
      "Batch 989: Invalid loss, terminating training\n",
      "Batch 990: Invalid loss, terminating training\n",
      "Batch 991: Invalid loss, terminating training\n",
      "Batch 992: Invalid loss, terminating training\n",
      "Batch 993: Invalid loss, terminating training\n",
      "Batch 994: Invalid loss, terminating training\n",
      "Batch 995: Invalid loss, terminating training\n",
      "Batch 996: Invalid loss, terminating training\n",
      "Batch 997: Invalid loss, terminating training\n",
      "Batch 998: Invalid loss, terminating training\n",
      "Batch 999: Invalid loss, terminating training\n",
      "Batch 1000: Invalid loss, terminating training\n",
      "Batch 1001: Invalid loss, terminating training\n",
      "Batch 1002: Invalid loss, terminating training\n",
      "Batch 1003: Invalid loss, terminating training\n",
      "Batch 1004: Invalid loss, terminating training\n",
      "Batch 1005: Invalid loss, terminating training\n",
      "Batch 1006: Invalid loss, terminating training\n",
      "Batch 1007: Invalid loss, terminating training\n",
      "Batch 1008: Invalid loss, terminating training\n",
      "Batch 1009: Invalid loss, terminating training\n",
      "Batch 1010: Invalid loss, terminating training\n",
      "Batch 1011: Invalid loss, terminating training\n",
      "Batch 1012: Invalid loss, terminating training\n",
      "Batch 1013: Invalid loss, terminating training\n",
      "Batch 1014: Invalid loss, terminating training\n",
      "Batch 1015: Invalid loss, terminating training\n",
      "1016/1981 [==============>...............] - ETA: 0s - loss: nan - rmsle: nanBatch 1016: Invalid loss, terminating training\n",
      "Batch 1017: Invalid loss, terminating training\n",
      "Batch 1018: Invalid loss, terminating training\n",
      "Batch 1019: Invalid loss, terminating training\n",
      "Batch 1020: Invalid loss, terminating training\n",
      "Batch 1021: Invalid loss, terminating training\n",
      "Batch 1022: Invalid loss, terminating training\n",
      "Batch 1023: Invalid loss, terminating training\n",
      "Batch 1024: Invalid loss, terminating training\n",
      "Batch 1025: Invalid loss, terminating training\n",
      "Batch 1026: Invalid loss, terminating training\n",
      "Batch 1027: Invalid loss, terminating training\n",
      "Batch 1028: Invalid loss, terminating training\n",
      "Batch 1029: Invalid loss, terminating training\n",
      "Batch 1030: Invalid loss, terminating training\n",
      "Batch 1031: Invalid loss, terminating training\n",
      "Batch 1032: Invalid loss, terminating training\n",
      "Batch 1033: Invalid loss, terminating training\n",
      "Batch 1034: Invalid loss, terminating training\n",
      "Batch 1035: Invalid loss, terminating training\n",
      "Batch 1036: Invalid loss, terminating training\n",
      "Batch 1037: Invalid loss, terminating training\n",
      "Batch 1038: Invalid loss, terminating training\n",
      "Batch 1039: Invalid loss, terminating training\n",
      "Batch 1040: Invalid loss, terminating training\n",
      "Batch 1041: Invalid loss, terminating training\n",
      "Batch 1042: Invalid loss, terminating training\n",
      "Batch 1043: Invalid loss, terminating training\n",
      "Batch 1044: Invalid loss, terminating training\n",
      "Batch 1045: Invalid loss, terminating training\n",
      "Batch 1046: Invalid loss, terminating training\n",
      "Batch 1047: Invalid loss, terminating training\n",
      "Batch 1048: Invalid loss, terminating training\n",
      "Batch 1049: Invalid loss, terminating training\n",
      "Batch 1050: Invalid loss, terminating training\n",
      "Batch 1051: Invalid loss, terminating training\n",
      "Batch 1052: Invalid loss, terminating training\n",
      "Batch 1053: Invalid loss, terminating training\n",
      "Batch 1054: Invalid loss, terminating training\n",
      "Batch 1055: Invalid loss, terminating training\n",
      "Batch 1056: Invalid loss, terminating training\n",
      "Batch 1057: Invalid loss, terminating training\n",
      "Batch 1058: Invalid loss, terminating training\n",
      "Batch 1059: Invalid loss, terminating training\n",
      "Batch 1060: Invalid loss, terminating training\n",
      "Batch 1061: Invalid loss, terminating training\n",
      "Batch 1062: Invalid loss, terminating training\n",
      "Batch 1063: Invalid loss, terminating training\n",
      "Batch 1064: Invalid loss, terminating training\n",
      "Batch 1065: Invalid loss, terminating training\n",
      "Batch 1066: Invalid loss, terminating training\n",
      "Batch 1067: Invalid loss, terminating training\n",
      "Batch 1068: Invalid loss, terminating training\n",
      "Batch 1069: Invalid loss, terminating training\n",
      "Batch 1070: Invalid loss, terminating training\n",
      "Batch 1071: Invalid loss, terminating training\n",
      "Batch 1072: Invalid loss, terminating training\n",
      "Batch 1073: Invalid loss, terminating training\n",
      "Batch 1074: Invalid loss, terminating training\n",
      "Batch 1075: Invalid loss, terminating training\n",
      "Batch 1076: Invalid loss, terminating training\n",
      "Batch 1077: Invalid loss, terminating training\n",
      "Batch 1078: Invalid loss, terminating training\n",
      "Batch 1079: Invalid loss, terminating training\n",
      "Batch 1080: Invalid loss, terminating training\n",
      "Batch 1081: Invalid loss, terminating training\n",
      "Batch 1082: Invalid loss, terminating training\n",
      "Batch 1083: Invalid loss, terminating training\n",
      "Batch 1084: Invalid loss, terminating training\n",
      "Batch 1085: Invalid loss, terminating training\n",
      "Batch 1086: Invalid loss, terminating training\n",
      "Batch 1087: Invalid loss, terminating training\n",
      "Batch 1088: Invalid loss, terminating training\n",
      "Batch 1089: Invalid loss, terminating training\n",
      "Batch 1090: Invalid loss, terminating training\n",
      "Batch 1091: Invalid loss, terminating training\n",
      "Batch 1092: Invalid loss, terminating training\n",
      "Batch 1093: Invalid loss, terminating training\n",
      "Batch 1094: Invalid loss, terminating training\n",
      "Batch 1095: Invalid loss, terminating training\n",
      "Batch 1096: Invalid loss, terminating training\n",
      "Batch 1097: Invalid loss, terminating training\n",
      "Batch 1098: Invalid loss, terminating training\n",
      "Batch 1099: Invalid loss, terminating training\n",
      "Batch 1100: Invalid loss, terminating training\n",
      "Batch 1101: Invalid loss, terminating training\n",
      "Batch 1102: Invalid loss, terminating training\n",
      "Batch 1103: Invalid loss, terminating training\n",
      "Batch 1104: Invalid loss, terminating training\n",
      "Batch 1105: Invalid loss, terminating training\n",
      "Batch 1106: Invalid loss, terminating training\n",
      "1107/1981 [===============>..............] - ETA: 0s - loss: nan - rmsle: nanBatch 1107: Invalid loss, terminating training\n",
      "Batch 1108: Invalid loss, terminating training\n",
      "Batch 1109: Invalid loss, terminating training\n",
      "Batch 1110: Invalid loss, terminating training\n",
      "Batch 1111: Invalid loss, terminating training\n",
      "Batch 1112: Invalid loss, terminating training\n",
      "Batch 1113: Invalid loss, terminating training\n",
      "Batch 1114: Invalid loss, terminating training\n",
      "Batch 1115: Invalid loss, terminating training\n",
      "Batch 1116: Invalid loss, terminating training\n",
      "Batch 1117: Invalid loss, terminating training\n",
      "Batch 1118: Invalid loss, terminating training\n",
      "Batch 1119: Invalid loss, terminating training\n",
      "Batch 1120: Invalid loss, terminating training\n",
      "Batch 1121: Invalid loss, terminating training\n",
      "Batch 1122: Invalid loss, terminating training\n",
      "Batch 1123: Invalid loss, terminating training\n",
      "Batch 1124: Invalid loss, terminating training\n",
      "Batch 1125: Invalid loss, terminating training\n",
      "Batch 1126: Invalid loss, terminating training\n",
      "Batch 1127: Invalid loss, terminating training\n",
      "Batch 1128: Invalid loss, terminating training\n",
      "Batch 1129: Invalid loss, terminating training\n",
      "Batch 1130: Invalid loss, terminating training\n",
      "Batch 1131: Invalid loss, terminating training\n",
      "Batch 1132: Invalid loss, terminating training\n",
      "Batch 1133: Invalid loss, terminating training\n",
      "Batch 1134: Invalid loss, terminating training\n",
      "Batch 1135: Invalid loss, terminating training\n",
      "Batch 1136: Invalid loss, terminating training\n",
      "Batch 1137: Invalid loss, terminating training\n",
      "Batch 1138: Invalid loss, terminating training\n",
      "Batch 1139: Invalid loss, terminating training\n",
      "Batch 1140: Invalid loss, terminating training\n",
      "Batch 1141: Invalid loss, terminating training\n",
      "Batch 1142: Invalid loss, terminating training\n",
      "Batch 1143: Invalid loss, terminating training\n",
      "Batch 1144: Invalid loss, terminating training\n",
      "Batch 1145: Invalid loss, terminating training\n",
      "Batch 1146: Invalid loss, terminating training\n",
      "Batch 1147: Invalid loss, terminating training\n",
      "Batch 1148: Invalid loss, terminating training\n",
      "Batch 1149: Invalid loss, terminating training\n",
      "Batch 1150: Invalid loss, terminating training\n",
      "Batch 1151: Invalid loss, terminating training\n",
      "Batch 1152: Invalid loss, terminating training\n",
      "Batch 1153: Invalid loss, terminating training\n",
      "Batch 1154: Invalid loss, terminating training\n",
      "Batch 1155: Invalid loss, terminating training\n",
      "Batch 1156: Invalid loss, terminating training\n",
      "Batch 1157: Invalid loss, terminating training\n",
      "Batch 1158: Invalid loss, terminating training\n",
      "Batch 1159: Invalid loss, terminating training\n",
      "Batch 1160: Invalid loss, terminating training\n",
      "Batch 1161: Invalid loss, terminating training\n",
      "Batch 1162: Invalid loss, terminating training\n",
      "Batch 1163: Invalid loss, terminating training\n",
      "Batch 1164: Invalid loss, terminating training\n",
      "Batch 1165: Invalid loss, terminating training\n",
      "Batch 1166: Invalid loss, terminating training\n",
      "Batch 1167: Invalid loss, terminating training\n",
      "Batch 1168: Invalid loss, terminating training\n",
      "Batch 1169: Invalid loss, terminating training\n",
      "Batch 1170: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171/1981 [================>.............] - ETA: 0s - loss: nan - rmsle: nanBatch 1171: Invalid loss, terminating training\n",
      "Batch 1172: Invalid loss, terminating training\n",
      "Batch 1173: Invalid loss, terminating training\n",
      "Batch 1174: Invalid loss, terminating training\n",
      "Batch 1175: Invalid loss, terminating training\n",
      "Batch 1176: Invalid loss, terminating training\n",
      "Batch 1177: Invalid loss, terminating training\n",
      "Batch 1178: Invalid loss, terminating training\n",
      "Batch 1179: Invalid loss, terminating training\n",
      "Batch 1180: Invalid loss, terminating training\n",
      "Batch 1181: Invalid loss, terminating training\n",
      "Batch 1182: Invalid loss, terminating training\n",
      "Batch 1183: Invalid loss, terminating training\n",
      "Batch 1184: Invalid loss, terminating training\n",
      "Batch 1185: Invalid loss, terminating training\n",
      "Batch 1186: Invalid loss, terminating training\n",
      "Batch 1187: Invalid loss, terminating training\n",
      "Batch 1188: Invalid loss, terminating training\n",
      "Batch 1189: Invalid loss, terminating training\n",
      "Batch 1190: Invalid loss, terminating training\n",
      "Batch 1191: Invalid loss, terminating training\n",
      "Batch 1192: Invalid loss, terminating training\n",
      "Batch 1193: Invalid loss, terminating training\n",
      "Batch 1194: Invalid loss, terminating training\n",
      "Batch 1195: Invalid loss, terminating training\n",
      "Batch 1196: Invalid loss, terminating training\n",
      "Batch 1197: Invalid loss, terminating training\n",
      "Batch 1198: Invalid loss, terminating training\n",
      "Batch 1199: Invalid loss, terminating training\n",
      "Batch 1200: Invalid loss, terminating training\n",
      "Batch 1201: Invalid loss, terminating training\n",
      "Batch 1202: Invalid loss, terminating training\n",
      "Batch 1203: Invalid loss, terminating training\n",
      "Batch 1204: Invalid loss, terminating training\n",
      "Batch 1205: Invalid loss, terminating training\n",
      "Batch 1206: Invalid loss, terminating training\n",
      "Batch 1207: Invalid loss, terminating training\n",
      "Batch 1208: Invalid loss, terminating training\n",
      "Batch 1209: Invalid loss, terminating training\n",
      "Batch 1210: Invalid loss, terminating training\n",
      "Batch 1211: Invalid loss, terminating training\n",
      "Batch 1212: Invalid loss, terminating training\n",
      "Batch 1213: Invalid loss, terminating training\n",
      "Batch 1214: Invalid loss, terminating training\n",
      "Batch 1215: Invalid loss, terminating training\n",
      "Batch 1216: Invalid loss, terminating training\n",
      "Batch 1217: Invalid loss, terminating training\n",
      "Batch 1218: Invalid loss, terminating training\n",
      "Batch 1219: Invalid loss, terminating training\n",
      "Batch 1220: Invalid loss, terminating training\n",
      "Batch 1221: Invalid loss, terminating training\n",
      "Batch 1222: Invalid loss, terminating training\n",
      "Batch 1223: Invalid loss, terminating training\n",
      "Batch 1224: Invalid loss, terminating training\n",
      "Batch 1225: Invalid loss, terminating training\n",
      "Batch 1226: Invalid loss, terminating training\n",
      "Batch 1227: Invalid loss, terminating training\n",
      "Batch 1228: Invalid loss, terminating training\n",
      "Batch 1229: Invalid loss, terminating training\n",
      "Batch 1230: Invalid loss, terminating training\n",
      "Batch 1231: Invalid loss, terminating training\n",
      "Batch 1232: Invalid loss, terminating training\n",
      "Batch 1233: Invalid loss, terminating training\n",
      "Batch 1234: Invalid loss, terminating training\n",
      "Batch 1235: Invalid loss, terminating training\n",
      "Batch 1236: Invalid loss, terminating training\n",
      "Batch 1237: Invalid loss, terminating training\n",
      "1238/1981 [=================>............] - ETA: 0s - loss: nan - rmsle: nanBatch 1238: Invalid loss, terminating training\n",
      "Batch 1239: Invalid loss, terminating training\n",
      "Batch 1240: Invalid loss, terminating training\n",
      "Batch 1241: Invalid loss, terminating training\n",
      "Batch 1242: Invalid loss, terminating training\n",
      "Batch 1243: Invalid loss, terminating training\n",
      "Batch 1244: Invalid loss, terminating training\n",
      "Batch 1245: Invalid loss, terminating training\n",
      "Batch 1246: Invalid loss, terminating training\n",
      "Batch 1247: Invalid loss, terminating training\n",
      "Batch 1248: Invalid loss, terminating training\n",
      "Batch 1249: Invalid loss, terminating training\n",
      "Batch 1250: Invalid loss, terminating training\n",
      "Batch 1251: Invalid loss, terminating training\n",
      "Batch 1252: Invalid loss, terminating training\n",
      "Batch 1253: Invalid loss, terminating training\n",
      "Batch 1254: Invalid loss, terminating training\n",
      "Batch 1255: Invalid loss, terminating training\n",
      "Batch 1256: Invalid loss, terminating training\n",
      "Batch 1257: Invalid loss, terminating training\n",
      "Batch 1258: Invalid loss, terminating training\n",
      "Batch 1259: Invalid loss, terminating training\n",
      "Batch 1260: Invalid loss, terminating training\n",
      "Batch 1261: Invalid loss, terminating training\n",
      "Batch 1262: Invalid loss, terminating training\n",
      "Batch 1263: Invalid loss, terminating training\n",
      "Batch 1264: Invalid loss, terminating training\n",
      "Batch 1265: Invalid loss, terminating training\n",
      "Batch 1266: Invalid loss, terminating training\n",
      "Batch 1267: Invalid loss, terminating training\n",
      "Batch 1268: Invalid loss, terminating training\n",
      "Batch 1269: Invalid loss, terminating training\n",
      "Batch 1270: Invalid loss, terminating training\n",
      "Batch 1271: Invalid loss, terminating training\n",
      "Batch 1272: Invalid loss, terminating training\n",
      "Batch 1273: Invalid loss, terminating training\n",
      "Batch 1274: Invalid loss, terminating training\n",
      "Batch 1275: Invalid loss, terminating training\n",
      "Batch 1276: Invalid loss, terminating training\n",
      "Batch 1277: Invalid loss, terminating training\n",
      "Batch 1278: Invalid loss, terminating training\n",
      "Batch 1279: Invalid loss, terminating training\n",
      "Batch 1280: Invalid loss, terminating training\n",
      "Batch 1281: Invalid loss, terminating training\n",
      "Batch 1282: Invalid loss, terminating training\n",
      "Batch 1283: Invalid loss, terminating training\n",
      "Batch 1284: Invalid loss, terminating training\n",
      "Batch 1285: Invalid loss, terminating training\n",
      "Batch 1286: Invalid loss, terminating training\n",
      "Batch 1287: Invalid loss, terminating training\n",
      "Batch 1288: Invalid loss, terminating training\n",
      "Batch 1289: Invalid loss, terminating training\n",
      "Batch 1290: Invalid loss, terminating training\n",
      "Batch 1291: Invalid loss, terminating training\n",
      "Batch 1292: Invalid loss, terminating training\n",
      "Batch 1293: Invalid loss, terminating training\n",
      "Batch 1294: Invalid loss, terminating training\n",
      "1295/1981 [==================>...........] - ETA: 0s - loss: nan - rmsle: nanBatch 1295: Invalid loss, terminating training\n",
      "Batch 1296: Invalid loss, terminating training\n",
      "Batch 1297: Invalid loss, terminating training\n",
      "Batch 1298: Invalid loss, terminating training\n",
      "Batch 1299: Invalid loss, terminating training\n",
      "Batch 1300: Invalid loss, terminating training\n",
      "Batch 1301: Invalid loss, terminating training\n",
      "Batch 1302: Invalid loss, terminating training\n",
      "Batch 1303: Invalid loss, terminating training\n",
      "Batch 1304: Invalid loss, terminating training\n",
      "Batch 1305: Invalid loss, terminating training\n",
      "Batch 1306: Invalid loss, terminating training\n",
      "Batch 1307: Invalid loss, terminating training\n",
      "Batch 1308: Invalid loss, terminating training\n",
      "Batch 1309: Invalid loss, terminating training\n",
      "Batch 1310: Invalid loss, terminating training\n",
      "Batch 1311: Invalid loss, terminating training\n",
      "Batch 1312: Invalid loss, terminating training\n",
      "Batch 1313: Invalid loss, terminating training\n",
      "Batch 1314: Invalid loss, terminating training\n",
      "Batch 1315: Invalid loss, terminating training\n",
      "Batch 1316: Invalid loss, terminating training\n",
      "Batch 1317: Invalid loss, terminating training\n",
      "Batch 1318: Invalid loss, terminating training\n",
      "Batch 1319: Invalid loss, terminating training\n",
      "Batch 1320: Invalid loss, terminating training\n",
      "Batch 1321: Invalid loss, terminating training\n",
      "Batch 1322: Invalid loss, terminating training\n",
      "Batch 1323: Invalid loss, terminating training\n",
      "Batch 1324: Invalid loss, terminating training\n",
      "Batch 1325: Invalid loss, terminating training\n",
      "Batch 1326: Invalid loss, terminating training\n",
      "Batch 1327: Invalid loss, terminating training\n",
      "Batch 1328: Invalid loss, terminating training\n",
      "Batch 1329: Invalid loss, terminating training\n",
      "Batch 1330: Invalid loss, terminating training\n",
      "Batch 1331: Invalid loss, terminating training\n",
      "Batch 1332: Invalid loss, terminating training\n",
      "Batch 1333: Invalid loss, terminating training\n",
      "Batch 1334: Invalid loss, terminating training\n",
      "Batch 1335: Invalid loss, terminating training\n",
      "Batch 1336: Invalid loss, terminating training\n",
      "Batch 1337: Invalid loss, terminating training\n",
      "Batch 1338: Invalid loss, terminating training\n",
      "Batch 1339: Invalid loss, terminating training\n",
      "Batch 1340: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1341: Invalid loss, terminating training\n",
      "1342/1981 [===================>..........] - ETA: 0s - loss: nan - rmsle: nanBatch 1342: Invalid loss, terminating training\n",
      "Batch 1343: Invalid loss, terminating training\n",
      "Batch 1344: Invalid loss, terminating training\n",
      "Batch 1345: Invalid loss, terminating training\n",
      "Batch 1346: Invalid loss, terminating training\n",
      "Batch 1347: Invalid loss, terminating training\n",
      "Batch 1348: Invalid loss, terminating training\n",
      "Batch 1349: Invalid loss, terminating training\n",
      "Batch 1350: Invalid loss, terminating training\n",
      "Batch 1351: Invalid loss, terminating training\n",
      "Batch 1352: Invalid loss, terminating training\n",
      "Batch 1353: Invalid loss, terminating training\n",
      "Batch 1354: Invalid loss, terminating training\n",
      "Batch 1355: Invalid loss, terminating training\n",
      "Batch 1356: Invalid loss, terminating training\n",
      "Batch 1357: Invalid loss, terminating training\n",
      "Batch 1358: Invalid loss, terminating training\n",
      "Batch 1359: Invalid loss, terminating training\n",
      "Batch 1360: Invalid loss, terminating training\n",
      "Batch 1361: Invalid loss, terminating training\n",
      "Batch 1362: Invalid loss, terminating training\n",
      "Batch 1363: Invalid loss, terminating training\n",
      "Batch 1364: Invalid loss, terminating training\n",
      "Batch 1365: Invalid loss, terminating training\n",
      "Batch 1366: Invalid loss, terminating training\n",
      "Batch 1367: Invalid loss, terminating training\n",
      "Batch 1368: Invalid loss, terminating training\n",
      "Batch 1369: Invalid loss, terminating training\n",
      "Batch 1370: Invalid loss, terminating training\n",
      "Batch 1371: Invalid loss, terminating training\n",
      "Batch 1372: Invalid loss, terminating training\n",
      "Batch 1373: Invalid loss, terminating training\n",
      "Batch 1374: Invalid loss, terminating training\n",
      "Batch 1375: Invalid loss, terminating training\n",
      "Batch 1376: Invalid loss, terminating training\n",
      "Batch 1377: Invalid loss, terminating training\n",
      "Batch 1378: Invalid loss, terminating training\n",
      "Batch 1379: Invalid loss, terminating training\n",
      "Batch 1380: Invalid loss, terminating training\n",
      "Batch 1381: Invalid loss, terminating training\n",
      "Batch 1382: Invalid loss, terminating training\n",
      "Batch 1383: Invalid loss, terminating training\n",
      "Batch 1384: Invalid loss, terminating training\n",
      "Batch 1385: Invalid loss, terminating training\n",
      "Batch 1386: Invalid loss, terminating training\n",
      "Batch 1387: Invalid loss, terminating training\n",
      "Batch 1388: Invalid loss, terminating training\n",
      "Batch 1389: Invalid loss, terminating training\n",
      "Batch 1390: Invalid loss, terminating training\n",
      "Batch 1391: Invalid loss, terminating training\n",
      "Batch 1392: Invalid loss, terminating training\n",
      "Batch 1393: Invalid loss, terminating training\n",
      "Batch 1394: Invalid loss, terminating training\n",
      "Batch 1395: Invalid loss, terminating training\n",
      "Batch 1396: Invalid loss, terminating training\n",
      "Batch 1397: Invalid loss, terminating training\n",
      "1398/1981 [====================>.........] - ETA: 0s - loss: nan - rmsle: nanBatch 1398: Invalid loss, terminating training\n",
      "Batch 1399: Invalid loss, terminating training\n",
      "Batch 1400: Invalid loss, terminating training\n",
      "Batch 1401: Invalid loss, terminating training\n",
      "Batch 1402: Invalid loss, terminating training\n",
      "Batch 1403: Invalid loss, terminating training\n",
      "Batch 1404: Invalid loss, terminating training\n",
      "Batch 1405: Invalid loss, terminating training\n",
      "Batch 1406: Invalid loss, terminating training\n",
      "Batch 1407: Invalid loss, terminating training\n",
      "Batch 1408: Invalid loss, terminating training\n",
      "Batch 1409: Invalid loss, terminating training\n",
      "Batch 1410: Invalid loss, terminating training\n",
      "Batch 1411: Invalid loss, terminating training\n",
      "Batch 1412: Invalid loss, terminating training\n",
      "Batch 1413: Invalid loss, terminating training\n",
      "Batch 1414: Invalid loss, terminating training\n",
      "Batch 1415: Invalid loss, terminating training\n",
      "Batch 1416: Invalid loss, terminating training\n",
      "Batch 1417: Invalid loss, terminating training\n",
      "Batch 1418: Invalid loss, terminating training\n",
      "Batch 1419: Invalid loss, terminating training\n",
      "Batch 1420: Invalid loss, terminating training\n",
      "Batch 1421: Invalid loss, terminating training\n",
      "Batch 1422: Invalid loss, terminating training\n",
      "Batch 1423: Invalid loss, terminating training\n",
      "Batch 1424: Invalid loss, terminating training\n",
      "Batch 1425: Invalid loss, terminating training\n",
      "Batch 1426: Invalid loss, terminating training\n",
      "Batch 1427: Invalid loss, terminating training\n",
      "Batch 1428: Invalid loss, terminating training\n",
      "Batch 1429: Invalid loss, terminating training\n",
      "Batch 1430: Invalid loss, terminating training\n",
      "Batch 1431: Invalid loss, terminating training\n",
      "Batch 1432: Invalid loss, terminating training\n",
      "Batch 1433: Invalid loss, terminating training\n",
      "Batch 1434: Invalid loss, terminating training\n",
      "Batch 1435: Invalid loss, terminating training\n",
      "Batch 1436: Invalid loss, terminating training\n",
      "Batch 1437: Invalid loss, terminating training\n",
      "Batch 1438: Invalid loss, terminating training\n",
      "Batch 1439: Invalid loss, terminating training\n",
      "Batch 1440: Invalid loss, terminating training\n",
      "Batch 1441: Invalid loss, terminating training\n",
      "Batch 1442: Invalid loss, terminating training\n",
      "1443/1981 [====================>.........] - ETA: 0s - loss: nan - rmsle: nanBatch 1443: Invalid loss, terminating training\n",
      "Batch 1444: Invalid loss, terminating training\n",
      "Batch 1445: Invalid loss, terminating training\n",
      "Batch 1446: Invalid loss, terminating training\n",
      "Batch 1447: Invalid loss, terminating training\n",
      "Batch 1448: Invalid loss, terminating training\n",
      "Batch 1449: Invalid loss, terminating training\n",
      "Batch 1450: Invalid loss, terminating training\n",
      "Batch 1451: Invalid loss, terminating training\n",
      "Batch 1452: Invalid loss, terminating training\n",
      "Batch 1453: Invalid loss, terminating training\n",
      "Batch 1454: Invalid loss, terminating training\n",
      "Batch 1455: Invalid loss, terminating training\n",
      "Batch 1456: Invalid loss, terminating training\n",
      "Batch 1457: Invalid loss, terminating training\n",
      "Batch 1458: Invalid loss, terminating training\n",
      "Batch 1459: Invalid loss, terminating training\n",
      "Batch 1460: Invalid loss, terminating training\n",
      "Batch 1461: Invalid loss, terminating training\n",
      "Batch 1462: Invalid loss, terminating training\n",
      "Batch 1463: Invalid loss, terminating training\n",
      "Batch 1464: Invalid loss, terminating training\n",
      "Batch 1465: Invalid loss, terminating training\n",
      "Batch 1466: Invalid loss, terminating training\n",
      "Batch 1467: Invalid loss, terminating training\n",
      "Batch 1468: Invalid loss, terminating training\n",
      "Batch 1469: Invalid loss, terminating training\n",
      "Batch 1470: Invalid loss, terminating training\n",
      "Batch 1471: Invalid loss, terminating training\n",
      "Batch 1472: Invalid loss, terminating training\n",
      "Batch 1473: Invalid loss, terminating training\n",
      "Batch 1474: Invalid loss, terminating training\n",
      "Batch 1475: Invalid loss, terminating training\n",
      "Batch 1476: Invalid loss, terminating training\n",
      "Batch 1477: Invalid loss, terminating training\n",
      "Batch 1478: Invalid loss, terminating training\n",
      "Batch 1479: Invalid loss, terminating training\n",
      "Batch 1480: Invalid loss, terminating training\n",
      "Batch 1481: Invalid loss, terminating training\n",
      "Batch 1482: Invalid loss, terminating training\n",
      "Batch 1483: Invalid loss, terminating training\n",
      "Batch 1484: Invalid loss, terminating training\n",
      "Batch 1485: Invalid loss, terminating training\n",
      "Batch 1486: Invalid loss, terminating training\n",
      "Batch 1487: Invalid loss, terminating training\n",
      "1488/1981 [=====================>........] - ETA: 0s - loss: nan - rmsle: nanBatch 1488: Invalid loss, terminating training\n",
      "Batch 1489: Invalid loss, terminating training\n",
      "Batch 1490: Invalid loss, terminating training\n",
      "Batch 1491: Invalid loss, terminating training\n",
      "Batch 1492: Invalid loss, terminating training\n",
      "Batch 1493: Invalid loss, terminating training\n",
      "Batch 1494: Invalid loss, terminating training\n",
      "Batch 1495: Invalid loss, terminating training\n",
      "Batch 1496: Invalid loss, terminating training\n",
      "Batch 1497: Invalid loss, terminating training\n",
      "Batch 1498: Invalid loss, terminating training\n",
      "Batch 1499: Invalid loss, terminating training\n",
      "Batch 1500: Invalid loss, terminating training\n",
      "Batch 1501: Invalid loss, terminating training\n",
      "Batch 1502: Invalid loss, terminating training\n",
      "Batch 1503: Invalid loss, terminating training\n",
      "Batch 1504: Invalid loss, terminating training\n",
      "Batch 1505: Invalid loss, terminating training\n",
      "Batch 1506: Invalid loss, terminating training\n",
      "Batch 1507: Invalid loss, terminating training\n",
      "Batch 1508: Invalid loss, terminating training\n",
      "Batch 1509: Invalid loss, terminating training\n",
      "Batch 1510: Invalid loss, terminating training\n",
      "Batch 1511: Invalid loss, terminating training\n",
      "Batch 1512: Invalid loss, terminating training\n",
      "Batch 1513: Invalid loss, terminating training\n",
      "Batch 1514: Invalid loss, terminating training\n",
      "Batch 1515: Invalid loss, terminating training\n",
      "Batch 1516: Invalid loss, terminating training\n",
      "Batch 1517: Invalid loss, terminating training\n",
      "Batch 1518: Invalid loss, terminating training\n",
      "Batch 1519: Invalid loss, terminating training\n",
      "Batch 1520: Invalid loss, terminating training\n",
      "Batch 1521: Invalid loss, terminating training\n",
      "Batch 1522: Invalid loss, terminating training\n",
      "Batch 1523: Invalid loss, terminating training\n",
      "Batch 1524: Invalid loss, terminating training\n",
      "Batch 1525: Invalid loss, terminating training\n",
      "Batch 1526: Invalid loss, terminating training\n",
      "Batch 1527: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1528: Invalid loss, terminating training\n",
      "Batch 1529: Invalid loss, terminating training\n",
      "Batch 1530: Invalid loss, terminating training\n",
      "1531/1981 [======================>.......] - ETA: 0s - loss: nan - rmsle: nanBatch 1531: Invalid loss, terminating training\n",
      "Batch 1532: Invalid loss, terminating training\n",
      "Batch 1533: Invalid loss, terminating training\n",
      "Batch 1534: Invalid loss, terminating training\n",
      "Batch 1535: Invalid loss, terminating training\n",
      "Batch 1536: Invalid loss, terminating training\n",
      "Batch 1537: Invalid loss, terminating training\n",
      "Batch 1538: Invalid loss, terminating training\n",
      "Batch 1539: Invalid loss, terminating training\n",
      "Batch 1540: Invalid loss, terminating training\n",
      "Batch 1541: Invalid loss, terminating training\n",
      "Batch 1542: Invalid loss, terminating training\n",
      "Batch 1543: Invalid loss, terminating training\n",
      "Batch 1544: Invalid loss, terminating training\n",
      "Batch 1545: Invalid loss, terminating training\n",
      "Batch 1546: Invalid loss, terminating training\n",
      "Batch 1547: Invalid loss, terminating training\n",
      "Batch 1548: Invalid loss, terminating training\n",
      "Batch 1549: Invalid loss, terminating training\n",
      "Batch 1550: Invalid loss, terminating training\n",
      "Batch 1551: Invalid loss, terminating training\n",
      "Batch 1552: Invalid loss, terminating training\n",
      "Batch 1553: Invalid loss, terminating training\n",
      "Batch 1554: Invalid loss, terminating training\n",
      "Batch 1555: Invalid loss, terminating training\n",
      "Batch 1556: Invalid loss, terminating training\n",
      "Batch 1557: Invalid loss, terminating training\n",
      "Batch 1558: Invalid loss, terminating training\n",
      "Batch 1559: Invalid loss, terminating training\n",
      "Batch 1560: Invalid loss, terminating training\n",
      "Batch 1561: Invalid loss, terminating training\n",
      "Batch 1562: Invalid loss, terminating training\n",
      "Batch 1563: Invalid loss, terminating training\n",
      "Batch 1564: Invalid loss, terminating training\n",
      "Batch 1565: Invalid loss, terminating training\n",
      "Batch 1566: Invalid loss, terminating training\n",
      "Batch 1567: Invalid loss, terminating training\n",
      "Batch 1568: Invalid loss, terminating training\n",
      "Batch 1569: Invalid loss, terminating training\n",
      "Batch 1570: Invalid loss, terminating training\n",
      "Batch 1571: Invalid loss, terminating training\n",
      "Batch 1572: Invalid loss, terminating training\n",
      "Batch 1573: Invalid loss, terminating training\n",
      "Batch 1574: Invalid loss, terminating training\n",
      "Batch 1575: Invalid loss, terminating training\n",
      "Batch 1576: Invalid loss, terminating training\n",
      "Batch 1577: Invalid loss, terminating training\n",
      "1578/1981 [======================>.......] - ETA: 0s - loss: nan - rmsle: nanBatch 1578: Invalid loss, terminating training\n",
      "Batch 1579: Invalid loss, terminating training\n",
      "Batch 1580: Invalid loss, terminating training\n",
      "Batch 1581: Invalid loss, terminating training\n",
      "Batch 1582: Invalid loss, terminating training\n",
      "Batch 1583: Invalid loss, terminating training\n",
      "Batch 1584: Invalid loss, terminating training\n",
      "Batch 1585: Invalid loss, terminating training\n",
      "Batch 1586: Invalid loss, terminating training\n",
      "Batch 1587: Invalid loss, terminating training\n",
      "Batch 1588: Invalid loss, terminating training\n",
      "Batch 1589: Invalid loss, terminating training\n",
      "Batch 1590: Invalid loss, terminating training\n",
      "Batch 1591: Invalid loss, terminating training\n",
      "Batch 1592: Invalid loss, terminating training\n",
      "Batch 1593: Invalid loss, terminating training\n",
      "Batch 1594: Invalid loss, terminating training\n",
      "Batch 1595: Invalid loss, terminating training\n",
      "Batch 1596: Invalid loss, terminating training\n",
      "Batch 1597: Invalid loss, terminating training\n",
      "Batch 1598: Invalid loss, terminating training\n",
      "Batch 1599: Invalid loss, terminating training\n",
      "Batch 1600: Invalid loss, terminating training\n",
      "Batch 1601: Invalid loss, terminating training\n",
      "Batch 1602: Invalid loss, terminating training\n",
      "Batch 1603: Invalid loss, terminating training\n",
      "Batch 1604: Invalid loss, terminating training\n",
      "Batch 1605: Invalid loss, terminating training\n",
      "Batch 1606: Invalid loss, terminating training\n",
      "Batch 1607: Invalid loss, terminating training\n",
      "Batch 1608: Invalid loss, terminating training\n",
      "Batch 1609: Invalid loss, terminating training\n",
      "Batch 1610: Invalid loss, terminating training\n",
      "Batch 1611: Invalid loss, terminating training\n",
      "Batch 1612: Invalid loss, terminating training\n",
      "Batch 1613: Invalid loss, terminating training\n",
      "Batch 1614: Invalid loss, terminating training\n",
      "Batch 1615: Invalid loss, terminating training\n",
      "Batch 1616: Invalid loss, terminating training\n",
      "Batch 1617: Invalid loss, terminating training\n",
      "Batch 1618: Invalid loss, terminating training\n",
      "Batch 1619: Invalid loss, terminating training\n",
      "Batch 1620: Invalid loss, terminating training\n",
      "Batch 1621: Invalid loss, terminating training\n",
      "Batch 1622: Invalid loss, terminating training\n",
      "Batch 1623: Invalid loss, terminating training\n",
      "Batch 1624: Invalid loss, terminating training\n",
      "Batch 1625: Invalid loss, terminating training\n",
      "Batch 1626: Invalid loss, terminating training\n",
      "Batch 1627: Invalid loss, terminating training\n",
      "Batch 1628: Invalid loss, terminating training\n",
      "Batch 1629: Invalid loss, terminating training\n",
      "Batch 1630: Invalid loss, terminating training\n",
      "Batch 1631: Invalid loss, terminating training\n",
      "Batch 1632: Invalid loss, terminating training\n",
      "Batch 1633: Invalid loss, terminating training\n",
      "Batch 1634: Invalid loss, terminating training\n",
      "Batch 1635: Invalid loss, terminating training\n",
      "1636/1981 [=======================>......] - ETA: 0s - loss: nan - rmsle: nanBatch 1636: Invalid loss, terminating training\n",
      "Batch 1637: Invalid loss, terminating training\n",
      "Batch 1638: Invalid loss, terminating training\n",
      "Batch 1639: Invalid loss, terminating training\n",
      "Batch 1640: Invalid loss, terminating training\n",
      "Batch 1641: Invalid loss, terminating training\n",
      "Batch 1642: Invalid loss, terminating training\n",
      "Batch 1643: Invalid loss, terminating training\n",
      "Batch 1644: Invalid loss, terminating training\n",
      "Batch 1645: Invalid loss, terminating training\n",
      "Batch 1646: Invalid loss, terminating training\n",
      "Batch 1647: Invalid loss, terminating training\n",
      "Batch 1648: Invalid loss, terminating training\n",
      "Batch 1649: Invalid loss, terminating training\n",
      "Batch 1650: Invalid loss, terminating training\n",
      "Batch 1651: Invalid loss, terminating training\n",
      "Batch 1652: Invalid loss, terminating training\n",
      "Batch 1653: Invalid loss, terminating training\n",
      "Batch 1654: Invalid loss, terminating training\n",
      "Batch 1655: Invalid loss, terminating training\n",
      "Batch 1656: Invalid loss, terminating training\n",
      "Batch 1657: Invalid loss, terminating training\n",
      "Batch 1658: Invalid loss, terminating training\n",
      "Batch 1659: Invalid loss, terminating training\n",
      "Batch 1660: Invalid loss, terminating training\n",
      "Batch 1661: Invalid loss, terminating training\n",
      "Batch 1662: Invalid loss, terminating training\n",
      "Batch 1663: Invalid loss, terminating training\n",
      "Batch 1664: Invalid loss, terminating training\n",
      "Batch 1665: Invalid loss, terminating training\n",
      "Batch 1666: Invalid loss, terminating training\n",
      "Batch 1667: Invalid loss, terminating training\n",
      "Batch 1668: Invalid loss, terminating training\n",
      "Batch 1669: Invalid loss, terminating training\n",
      "Batch 1670: Invalid loss, terminating training\n",
      "Batch 1671: Invalid loss, terminating training\n",
      "Batch 1672: Invalid loss, terminating training\n",
      "Batch 1673: Invalid loss, terminating training\n",
      "Batch 1674: Invalid loss, terminating training\n",
      "Batch 1675: Invalid loss, terminating training\n",
      "Batch 1676: Invalid loss, terminating training\n",
      "Batch 1677: Invalid loss, terminating training\n",
      "Batch 1678: Invalid loss, terminating training\n",
      "Batch 1679: Invalid loss, terminating training\n",
      "1680/1981 [========================>.....] - ETA: 0s - loss: nan - rmsle: nanBatch 1680: Invalid loss, terminating training\n",
      "Batch 1681: Invalid loss, terminating training\n",
      "Batch 1682: Invalid loss, terminating training\n",
      "Batch 1683: Invalid loss, terminating training\n",
      "Batch 1684: Invalid loss, terminating training\n",
      "Batch 1685: Invalid loss, terminating training\n",
      "Batch 1686: Invalid loss, terminating training\n",
      "Batch 1687: Invalid loss, terminating training\n",
      "Batch 1688: Invalid loss, terminating training\n",
      "Batch 1689: Invalid loss, terminating training\n",
      "Batch 1690: Invalid loss, terminating training\n",
      "Batch 1691: Invalid loss, terminating training\n",
      "Batch 1692: Invalid loss, terminating training\n",
      "Batch 1693: Invalid loss, terminating training\n",
      "Batch 1694: Invalid loss, terminating training\n",
      "Batch 1695: Invalid loss, terminating training\n",
      "Batch 1696: Invalid loss, terminating training\n",
      "Batch 1697: Invalid loss, terminating training\n",
      "Batch 1698: Invalid loss, terminating training\n",
      "Batch 1699: Invalid loss, terminating training\n",
      "Batch 1700: Invalid loss, terminating training\n",
      "Batch 1701: Invalid loss, terminating training\n",
      "Batch 1702: Invalid loss, terminating training\n",
      "Batch 1703: Invalid loss, terminating training\n",
      "Batch 1704: Invalid loss, terminating training\n",
      "Batch 1705: Invalid loss, terminating training\n",
      "Batch 1706: Invalid loss, terminating training\n",
      "Batch 1707: Invalid loss, terminating training\n",
      "Batch 1708: Invalid loss, terminating training\n",
      "Batch 1709: Invalid loss, terminating training\n",
      "Batch 1710: Invalid loss, terminating training\n",
      "Batch 1711: Invalid loss, terminating training\n",
      "Batch 1712: Invalid loss, terminating training\n",
      "Batch 1713: Invalid loss, terminating training\n",
      "Batch 1714: Invalid loss, terminating training\n",
      "Batch 1715: Invalid loss, terminating training\n",
      "Batch 1716: Invalid loss, terminating training\n",
      "Batch 1717: Invalid loss, terminating training\n",
      "Batch 1718: Invalid loss, terminating training\n",
      "Batch 1719: Invalid loss, terminating training\n",
      "Batch 1720: Invalid loss, terminating training\n",
      "Batch 1721: Invalid loss, terminating training\n",
      "Batch 1722: Invalid loss, terminating training\n",
      "Batch 1723: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724/1981 [=========================>....] - ETA: 0s - loss: nan - rmsle: nanBatch 1724: Invalid loss, terminating training\n",
      "Batch 1725: Invalid loss, terminating training\n",
      "Batch 1726: Invalid loss, terminating training\n",
      "Batch 1727: Invalid loss, terminating training\n",
      "Batch 1728: Invalid loss, terminating training\n",
      "Batch 1729: Invalid loss, terminating training\n",
      "Batch 1730: Invalid loss, terminating training\n",
      "Batch 1731: Invalid loss, terminating training\n",
      "Batch 1732: Invalid loss, terminating training\n",
      "Batch 1733: Invalid loss, terminating training\n",
      "Batch 1734: Invalid loss, terminating training\n",
      "Batch 1735: Invalid loss, terminating training\n",
      "Batch 1736: Invalid loss, terminating training\n",
      "Batch 1737: Invalid loss, terminating training\n",
      "Batch 1738: Invalid loss, terminating training\n",
      "Batch 1739: Invalid loss, terminating training\n",
      "Batch 1740: Invalid loss, terminating training\n",
      "Batch 1741: Invalid loss, terminating training\n",
      "Batch 1742: Invalid loss, terminating training\n",
      "Batch 1743: Invalid loss, terminating training\n",
      "Batch 1744: Invalid loss, terminating training\n",
      "Batch 1745: Invalid loss, terminating training\n",
      "Batch 1746: Invalid loss, terminating training\n",
      "Batch 1747: Invalid loss, terminating training\n",
      "Batch 1748: Invalid loss, terminating training\n",
      "Batch 1749: Invalid loss, terminating training\n",
      "Batch 1750: Invalid loss, terminating training\n",
      "Batch 1751: Invalid loss, terminating training\n",
      "Batch 1752: Invalid loss, terminating training\n",
      "Batch 1753: Invalid loss, terminating training\n",
      "Batch 1754: Invalid loss, terminating training\n",
      "Batch 1755: Invalid loss, terminating training\n",
      "Batch 1756: Invalid loss, terminating training\n",
      "Batch 1757: Invalid loss, terminating training\n",
      "Batch 1758: Invalid loss, terminating training\n",
      "Batch 1759: Invalid loss, terminating training\n",
      "Batch 1760: Invalid loss, terminating training\n",
      "Batch 1761: Invalid loss, terminating training\n",
      "Batch 1762: Invalid loss, terminating training\n",
      "Batch 1763: Invalid loss, terminating training\n",
      "1764/1981 [=========================>....] - ETA: 0s - loss: nan - rmsle: nanBatch 1764: Invalid loss, terminating training\n",
      "Batch 1765: Invalid loss, terminating training\n",
      "Batch 1766: Invalid loss, terminating training\n",
      "Batch 1767: Invalid loss, terminating training\n",
      "Batch 1768: Invalid loss, terminating training\n",
      "Batch 1769: Invalid loss, terminating training\n",
      "Batch 1770: Invalid loss, terminating training\n",
      "Batch 1771: Invalid loss, terminating training\n",
      "Batch 1772: Invalid loss, terminating training\n",
      "Batch 1773: Invalid loss, terminating training\n",
      "Batch 1774: Invalid loss, terminating training\n",
      "Batch 1775: Invalid loss, terminating training\n",
      "Batch 1776: Invalid loss, terminating training\n",
      "Batch 1777: Invalid loss, terminating training\n",
      "Batch 1778: Invalid loss, terminating training\n",
      "Batch 1779: Invalid loss, terminating training\n",
      "Batch 1780: Invalid loss, terminating training\n",
      "Batch 1781: Invalid loss, terminating training\n",
      "Batch 1782: Invalid loss, terminating training\n",
      "Batch 1783: Invalid loss, terminating training\n",
      "Batch 1784: Invalid loss, terminating training\n",
      "Batch 1785: Invalid loss, terminating training\n",
      "Batch 1786: Invalid loss, terminating training\n",
      "Batch 1787: Invalid loss, terminating training\n",
      "Batch 1788: Invalid loss, terminating training\n",
      "Batch 1789: Invalid loss, terminating training\n",
      "Batch 1790: Invalid loss, terminating training\n",
      "Batch 1791: Invalid loss, terminating training\n",
      "Batch 1792: Invalid loss, terminating training\n",
      "Batch 1793: Invalid loss, terminating training\n",
      "Batch 1794: Invalid loss, terminating training\n",
      "Batch 1795: Invalid loss, terminating training\n",
      "Batch 1796: Invalid loss, terminating training\n",
      "Batch 1797: Invalid loss, terminating training\n",
      "Batch 1798: Invalid loss, terminating training\n",
      "Batch 1799: Invalid loss, terminating training\n",
      "Batch 1800: Invalid loss, terminating training\n",
      "Batch 1801: Invalid loss, terminating training\n",
      "Batch 1802: Invalid loss, terminating training\n",
      "Batch 1803: Invalid loss, terminating training\n",
      "Batch 1804: Invalid loss, terminating training\n",
      "Batch 1805: Invalid loss, terminating training\n",
      "Batch 1806: Invalid loss, terminating training\n",
      "Batch 1807: Invalid loss, terminating training\n",
      "Batch 1808: Invalid loss, terminating training\n",
      "Batch 1809: Invalid loss, terminating training\n",
      "Batch 1810: Invalid loss, terminating training\n",
      "1811/1981 [==========================>...] - ETA: 0s - loss: nan - rmsle: nanBatch 1811: Invalid loss, terminating training\n",
      "Batch 1812: Invalid loss, terminating training\n",
      "Batch 1813: Invalid loss, terminating training\n",
      "Batch 1814: Invalid loss, terminating training\n",
      "Batch 1815: Invalid loss, terminating training\n",
      "Batch 1816: Invalid loss, terminating training\n",
      "Batch 1817: Invalid loss, terminating training\n",
      "Batch 1818: Invalid loss, terminating training\n",
      "Batch 1819: Invalid loss, terminating training\n",
      "Batch 1820: Invalid loss, terminating training\n",
      "Batch 1821: Invalid loss, terminating training\n",
      "Batch 1822: Invalid loss, terminating training\n",
      "Batch 1823: Invalid loss, terminating training\n",
      "Batch 1824: Invalid loss, terminating training\n",
      "Batch 1825: Invalid loss, terminating training\n",
      "Batch 1826: Invalid loss, terminating training\n",
      "Batch 1827: Invalid loss, terminating training\n",
      "Batch 1828: Invalid loss, terminating training\n",
      "Batch 1829: Invalid loss, terminating training\n",
      "Batch 1830: Invalid loss, terminating training\n",
      "Batch 1831: Invalid loss, terminating training\n",
      "Batch 1832: Invalid loss, terminating training\n",
      "Batch 1833: Invalid loss, terminating training\n",
      "Batch 1834: Invalid loss, terminating training\n",
      "Batch 1835: Invalid loss, terminating training\n",
      "Batch 1836: Invalid loss, terminating training\n",
      "Batch 1837: Invalid loss, terminating training\n",
      "Batch 1838: Invalid loss, terminating training\n",
      "Batch 1839: Invalid loss, terminating training\n",
      "Batch 1840: Invalid loss, terminating training\n",
      "Batch 1841: Invalid loss, terminating training\n",
      "Batch 1842: Invalid loss, terminating training\n",
      "Batch 1843: Invalid loss, terminating training\n",
      "Batch 1844: Invalid loss, terminating training\n",
      "Batch 1845: Invalid loss, terminating training\n",
      "Batch 1846: Invalid loss, terminating training\n",
      "Batch 1847: Invalid loss, terminating training\n",
      "Batch 1848: Invalid loss, terminating training\n",
      "Batch 1849: Invalid loss, terminating training\n",
      "Batch 1850: Invalid loss, terminating training\n",
      "Batch 1851: Invalid loss, terminating training\n",
      "Batch 1852: Invalid loss, terminating training\n",
      "Batch 1853: Invalid loss, terminating training\n",
      "Batch 1854: Invalid loss, terminating training\n",
      "Batch 1855: Invalid loss, terminating training\n",
      "Batch 1856: Invalid loss, terminating training\n",
      "Batch 1857: Invalid loss, terminating training\n",
      "Batch 1858: Invalid loss, terminating training\n",
      "Batch 1859: Invalid loss, terminating training\n",
      "1860/1981 [===========================>..] - ETA: 0s - loss: nan - rmsle: nanBatch 1860: Invalid loss, terminating training\n",
      "Batch 1861: Invalid loss, terminating training\n",
      "Batch 1862: Invalid loss, terminating training\n",
      "Batch 1863: Invalid loss, terminating training\n",
      "Batch 1864: Invalid loss, terminating training\n",
      "Batch 1865: Invalid loss, terminating training\n",
      "Batch 1866: Invalid loss, terminating training\n",
      "Batch 1867: Invalid loss, terminating training\n",
      "Batch 1868: Invalid loss, terminating training\n",
      "Batch 1869: Invalid loss, terminating training\n",
      "Batch 1870: Invalid loss, terminating training\n",
      "Batch 1871: Invalid loss, terminating training\n",
      "Batch 1872: Invalid loss, terminating training\n",
      "Batch 1873: Invalid loss, terminating training\n",
      "Batch 1874: Invalid loss, terminating training\n",
      "Batch 1875: Invalid loss, terminating training\n",
      "Batch 1876: Invalid loss, terminating training\n",
      "Batch 1877: Invalid loss, terminating training\n",
      "Batch 1878: Invalid loss, terminating training\n",
      "Batch 1879: Invalid loss, terminating training\n",
      "Batch 1880: Invalid loss, terminating training\n",
      "Batch 1881: Invalid loss, terminating training\n",
      "Batch 1882: Invalid loss, terminating training\n",
      "Batch 1883: Invalid loss, terminating training\n",
      "Batch 1884: Invalid loss, terminating training\n",
      "Batch 1885: Invalid loss, terminating training\n",
      "Batch 1886: Invalid loss, terminating training\n",
      "Batch 1887: Invalid loss, terminating training\n",
      "Batch 1888: Invalid loss, terminating training\n",
      "Batch 1889: Invalid loss, terminating training\n",
      "Batch 1890: Invalid loss, terminating training\n",
      "Batch 1891: Invalid loss, terminating training\n",
      "Batch 1892: Invalid loss, terminating training\n",
      "Batch 1893: Invalid loss, terminating training\n",
      "Batch 1894: Invalid loss, terminating training\n",
      "Batch 1895: Invalid loss, terminating training\n",
      "Batch 1896: Invalid loss, terminating training\n",
      "Batch 1897: Invalid loss, terminating training\n",
      "Batch 1898: Invalid loss, terminating training\n",
      "Batch 1899: Invalid loss, terminating training\n",
      "Batch 1900: Invalid loss, terminating training\n",
      "Batch 1901: Invalid loss, terminating training\n",
      "Batch 1902: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1903/1981 [===========================>..] - ETA: 0s - loss: nan - rmsle: nanBatch 1903: Invalid loss, terminating training\n",
      "Batch 1904: Invalid loss, terminating training\n",
      "Batch 1905: Invalid loss, terminating training\n",
      "Batch 1906: Invalid loss, terminating training\n",
      "Batch 1907: Invalid loss, terminating training\n",
      "Batch 1908: Invalid loss, terminating training\n",
      "Batch 1909: Invalid loss, terminating training\n",
      "Batch 1910: Invalid loss, terminating training\n",
      "Batch 1911: Invalid loss, terminating training\n",
      "Batch 1912: Invalid loss, terminating training\n",
      "Batch 1913: Invalid loss, terminating training\n",
      "Batch 1914: Invalid loss, terminating training\n",
      "Batch 1915: Invalid loss, terminating training\n",
      "Batch 1916: Invalid loss, terminating training\n",
      "Batch 1917: Invalid loss, terminating training\n",
      "Batch 1918: Invalid loss, terminating training\n",
      "Batch 1919: Invalid loss, terminating training\n",
      "Batch 1920: Invalid loss, terminating training\n",
      "Batch 1921: Invalid loss, terminating training\n",
      "Batch 1922: Invalid loss, terminating training\n",
      "Batch 1923: Invalid loss, terminating training\n",
      "Batch 1924: Invalid loss, terminating training\n",
      "Batch 1925: Invalid loss, terminating training\n",
      "Batch 1926: Invalid loss, terminating training\n",
      "Batch 1927: Invalid loss, terminating training\n",
      "Batch 1928: Invalid loss, terminating training\n",
      "Batch 1929: Invalid loss, terminating training\n",
      "Batch 1930: Invalid loss, terminating training\n",
      "Batch 1931: Invalid loss, terminating training\n",
      "Batch 1932: Invalid loss, terminating training\n",
      "Batch 1933: Invalid loss, terminating training\n",
      "Batch 1934: Invalid loss, terminating training\n",
      "Batch 1935: Invalid loss, terminating training\n",
      "Batch 1936: Invalid loss, terminating training\n",
      "Batch 1937: Invalid loss, terminating training\n",
      "Batch 1938: Invalid loss, terminating training\n",
      "Batch 1939: Invalid loss, terminating training\n",
      "Batch 1940: Invalid loss, terminating training\n",
      "Batch 1941: Invalid loss, terminating training\n",
      "Batch 1942: Invalid loss, terminating training\n",
      "Batch 1943: Invalid loss, terminating training\n",
      "1944/1981 [============================>.] - ETA: 0s - loss: nan - rmsle: nanBatch 1944: Invalid loss, terminating training\n",
      "Batch 1945: Invalid loss, terminating training\n",
      "Batch 1946: Invalid loss, terminating training\n",
      "Batch 1947: Invalid loss, terminating training\n",
      "Batch 1948: Invalid loss, terminating training\n",
      "Batch 1949: Invalid loss, terminating training\n",
      "Batch 1950: Invalid loss, terminating training\n",
      "Batch 1951: Invalid loss, terminating training\n",
      "Batch 1952: Invalid loss, terminating training\n",
      "Batch 1953: Invalid loss, terminating training\n",
      "Batch 1954: Invalid loss, terminating training\n",
      "Batch 1955: Invalid loss, terminating training\n",
      "Batch 1956: Invalid loss, terminating training\n",
      "Batch 1957: Invalid loss, terminating training\n",
      "Batch 1958: Invalid loss, terminating training\n",
      "Batch 1959: Invalid loss, terminating training\n",
      "Batch 1960: Invalid loss, terminating training\n",
      "Batch 1961: Invalid loss, terminating training\n",
      "Batch 1962: Invalid loss, terminating training\n",
      "Batch 1963: Invalid loss, terminating training\n",
      "Batch 1964: Invalid loss, terminating training\n",
      "Batch 1965: Invalid loss, terminating training\n",
      "Batch 1966: Invalid loss, terminating training\n",
      "Batch 1967: Invalid loss, terminating training\n",
      "Batch 1968: Invalid loss, terminating training\n",
      "Batch 1969: Invalid loss, terminating training\n",
      "Batch 1970: Invalid loss, terminating training\n",
      "Batch 1971: Invalid loss, terminating training\n",
      "Batch 1972: Invalid loss, terminating training\n",
      "Batch 1973: Invalid loss, terminating training\n",
      "Batch 1974: Invalid loss, terminating training\n",
      "Batch 1975: Invalid loss, terminating training\n",
      "Batch 1976: Invalid loss, terminating training\n",
      "Batch 1977: Invalid loss, terminating training\n",
      "Batch 1978: Invalid loss, terminating training\n",
      "Batch 1979: Invalid loss, terminating training\n",
      "Batch 1980: Invalid loss, terminating training\n",
      "1981/1981 [==============================] - 2s 997us/step - loss: nan - rmsle: nan - val_loss: nan - val_rmsle: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">60</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">58 # 모델 평가</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 </span>y_pred = model.predict(X_test)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>60 mae = mean_absolute_error(y_test, y_pred)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 </span>rmsle_score = np.sqrt(mean_squared_log_error(y_test, y_pred))                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">62 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'RMSLE on Test Data: {</span>rmsle_score<span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">63 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\metrics\\_regression.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">196</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">mean_absolute_error</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 193 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">&gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 194 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">0.85...</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 195 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 196 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>y_type, y_true, y_pred, multioutput = _check_reg_targets(                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>y_true, y_pred, multioutput                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>check_consistent_length(y_true, y_pred, sample_weight)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\metrics\\_regression.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">102</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_check_reg_targets</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  99 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 100 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>check_consistent_length(y_true, y_pred)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 101 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>y_true = check_array(y_true, ensure_2d=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>, dtype=dtype)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>y_pred = check_array(y_pred, ensure_2d=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>, dtype=dtype)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 103 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 104 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> y_true.ndim == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 105 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>y_true = y_true.reshape((-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>))                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\utils\\validation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">921</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">check_array</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 918 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 919 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 920 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> force_all_finite:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 921 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>_assert_all_finite(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 922 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>array,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 923 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>input_name=input_name,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 924 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>estimator_name=estimator_name,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\utils\\validation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">161</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_assert_all_finite</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 158 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" https://scikit-learn.org/stable/modules/impute.html\"</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 159 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"#estimators-that-handle-nan-values\"</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 160 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 161 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(msg_err)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 162 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 163 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 164 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">assert_all_finite</span>(                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Input contains NaN.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m60\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m57 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m# 모델 평가\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0my_pred = model.predict(X_test)                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m60 mae = mean_absolute_error(y_test, y_pred)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m61 \u001b[0mrmsle_score = np.sqrt(mean_squared_log_error(y_test, y_pred))                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m62 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mRMSLE on Test Data: \u001b[0m\u001b[33m{\u001b[0mrmsle_score\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m63 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m:\u001b[94m196\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mmean_absolute_error\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 193 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 194 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m0.85...\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 195 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 196 \u001b[2m│   \u001b[0my_type, y_true, y_pred, multioutput = _check_reg_targets(                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 197 \u001b[0m\u001b[2m│   │   \u001b[0my_true, y_pred, multioutput                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 198 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 199 \u001b[0m\u001b[2m│   \u001b[0mcheck_consistent_length(y_true, y_pred, sample_weight)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m:\u001b[94m102\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_check_reg_targets\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  99 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 100 \u001b[0m\u001b[2m│   \u001b[0mcheck_consistent_length(y_true, y_pred)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 101 \u001b[0m\u001b[2m│   \u001b[0my_true = check_array(y_true, ensure_2d=\u001b[94mFalse\u001b[0m, dtype=dtype)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 102 \u001b[2m│   \u001b[0my_pred = check_array(y_pred, ensure_2d=\u001b[94mFalse\u001b[0m, dtype=dtype)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 103 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 104 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m y_true.ndim == \u001b[94m1\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 105 \u001b[0m\u001b[2m│   │   \u001b[0my_true = y_true.reshape((-\u001b[94m1\u001b[0m, \u001b[94m1\u001b[0m))                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m:\u001b[94m921\u001b[0m in        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mcheck_array\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 920 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m force_all_finite:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 921 \u001b[2m│   │   │   \u001b[0m_assert_all_finite(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   │   │   \u001b[0marray,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_name=input_name,                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 924 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mestimator_name=estimator_name,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\User\\anaconda3\\envs\\project\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m:\u001b[94m161\u001b[0m in        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_assert_all_finite\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 158 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m https://scikit-learn.org/stable/modules/impute.html\u001b[0m\u001b[33m\"\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 159 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m#estimators-that-handle-nan-values\u001b[0m\u001b[33m\"\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 160 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 161 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(msg_err)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 162 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 163 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92massert_all_finite\u001b[0m(                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mInput contains NaN.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 특성과 타겟 데이터 분리\n",
    "X = train.drop('ECLO',axis=1)\n",
    "y = train['ECLO']\n",
    "\n",
    "X = X.astype('float64')\n",
    "y = y.astype('float64')\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RMSLE 손실 함수 정의\n",
    "def rmsle(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(y_true) - tf.math.log1p(y_pred))))\n",
    "\n",
    "# 시퀀셜 모델 구축\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_dim=X_train.shape[1], dtype='float32'),\n",
    "#     BatchNormalization(),\n",
    "    tf.keras.layers.Dense(16, activation='relu', dtype='float32'),\n",
    "#     BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='linear', dtype='float32')\n",
    "])\n",
    "\n",
    "# 초기 학습률 설정\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "# 학습률 스케줄링 설정\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100,  # 학습률을 얼마나 자주 감소시킬지 결정\n",
    "    decay_rate=0.1,     # 감소 비율\n",
    "    staircase=True)\n",
    "\n",
    "# 옵티마이저에 학습률 스케줄 적용\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=optimizer, loss=rmsle, metrics=[rmsle])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test)\n",
    "                  , callbacks=[early_stopping, terminate_on_nan])\n",
    "\n",
    "# 훈련 과정 시각화\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmsle_score = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "print(f'RMSLE on Test Data: {rmsle_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4b348874",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ECLO'\n",
    "light_df = pd.read_csv('./open/external_open/대구 보안등 정보.csv', encoding='cp949')[['설치개수', '소재지지번주소']]\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "light_df[['도시', '구', '동', '번지']] = light_df['소재지지번주소'].str.extract(location_pattern)\n",
    "light_df = light_df.drop(columns=['소재지지번주소', '번지'])\n",
    "light_df = light_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "light_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "child_area_df = pd.read_csv('./open/external_open/대구 어린이 보호 구역 정보.csv', encoding='cp949').drop_duplicates()# [['소재지지번주소']]\n",
    "child_area_df['시설수'] = 1\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "child_area_df[['도시', '구', '동', '번지']] = child_area_df['소재지지번주소'].str.extract(location_pattern)\n",
    "child_area_df = child_area_df.drop(columns=['소재지지번주소', '번지','위도','경도'])\n",
    "child_area_df = child_area_df.groupby(['도시', '구', '동']).sum().reset_index()\n",
    "child_area_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "parking_df = pd.read_csv('./open/external_open/대구 주차장 정보.csv', encoding='cp949')\n",
    "parking_df = pd.get_dummies(parking_df, columns=['급지구분'])\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
    "parking_df[['도시', '구', '동', '번지']] = parking_df['소재지지번주소'].str.extract(location_pattern)\n",
    "parking_df = parking_df.drop(columns=['소재지지번주소', '번지','위도','경도'])\n",
    "parking_df = parking_df.groupby(['도시', '구', '동']).mean().reset_index()\n",
    "parking_df.reset_index(inplace=True, drop=True)\n",
    "parking_df = parking_df.fillna(0)\n",
    "\n",
    "train = pd.read_csv('./open/train.csv')\n",
    "test = pd.read_csv('./open/test.csv')\n",
    "cols = list(test.columns)\n",
    "cols.append(target)\n",
    "train = train[cols]\n",
    "\n",
    "time_pattern = r'(\\d{4})-(\\d{1,2})-(\\d{1,2}) (\\d{1,2})' \n",
    "train[['연', '월', '일', '시간']] = train['사고일시'].str.extract(time_pattern)\n",
    "train[['연', '월', '일', '시간']] = train[['연', '월', '일', '시간']].apply(pd.to_numeric) \n",
    "train = train.drop(columns=['사고일시'])\n",
    "test[['연', '월', '일', '시간']] = test['사고일시'].str.extract(time_pattern)\n",
    "test[['연', '월', '일', '시간']] = test[['연', '월', '일', '시간']].apply(pd.to_numeric)\n",
    "test = test.drop(columns=['사고일시'])\n",
    "\n",
    "location_pattern = r'(\\S+) (\\S+) (\\S+)'\n",
    "train[['도시', '구', '동']] = train['시군구'].str.extract(location_pattern)\n",
    "train = train.drop(columns=['시군구'])\n",
    "test[['도시', '구', '동']] = test['시군구'].str.extract(location_pattern)\n",
    "test = test.drop(columns=['시군구'])\n",
    "\n",
    "road_pattern = r'(.+) - (.+)'\n",
    "train[['도로형태1', '도로형태2']] = train['도로형태'].str.extract(road_pattern)\n",
    "train = train.drop(columns=['도로형태'])\n",
    "\n",
    "test[['도로형태1', '도로형태2']] = test['도로형태'].str.extract(road_pattern)\n",
    "test = test.drop(columns=['도로형태'])\n",
    "\n",
    "train = pd.merge(train, light_df, how='left', on=['도시', '구', '동'])\n",
    "train = pd.merge(train, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "train = pd.merge(train, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "test = pd.merge(test, light_df, how='left', on=['도시', '구', '동'])\n",
    "test = pd.merge(test, child_area_df, how='left', on=['도시', '구', '동'])\n",
    "test = pd.merge(test, parking_df, how='left', on=['도시', '구', '동'])\n",
    "\n",
    "train = train.drop('ID',axis=1)\n",
    "test = test.drop('ID',axis=1)\n",
    "# train = train.dropna()\n",
    "\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "# test_x = test.copy()\n",
    "# train_x = train[test.columns].copy()\n",
    "# train_y = train['ECLO'].copy()\n",
    "categorical_features = list(train.dtypes[train.dtypes == \"object\"].index)\n",
    "for i in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train[i] = le.fit_transform(train[i])\n",
    "    test[i] = le.transform(test[i])\n",
    "    \n",
    "# minmax_scaler = MinMaxScaler()\n",
    "# data = minmax_scaler.fit_transform(train.drop('ECLO',axis=1))\n",
    "# data = pd.DataFrame(data,columns=train.drop('ECLO',axis=1).columns)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5f1b739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 261532.36653002\n",
      "Validation score: -15.070912\n",
      "Iteration 2, loss = 45.43708394\n",
      "Validation score: -3.940771\n",
      "Iteration 3, loss = 25.76355211\n",
      "Validation score: -8.897161\n",
      "Iteration 4, loss = 32.15018503\n",
      "Validation score: -2.737772\n",
      "Iteration 5, loss = 15.47314428\n",
      "Validation score: -1.433850\n",
      "Iteration 6, loss = 12.97435224\n",
      "Validation score: -1.169729\n",
      "Iteration 7, loss = 36.27204762\n",
      "Validation score: -2.493052\n",
      "Iteration 8, loss = 236.31131224\n",
      "Validation score: -22.417561\n",
      "Iteration 9, loss = 1709.15108224\n",
      "Validation score: -181.405654\n",
      "Iteration 10, loss = 2137.01569833\n",
      "Validation score: -8.223862\n",
      "Iteration 11, loss = 2770.29678328\n",
      "Validation score: -5030.638529\n",
      "Iteration 12, loss = 7606.42903329\n",
      "Validation score: -2.353304\n",
      "Iteration 13, loss = 14.67760743\n",
      "Validation score: -0.692514\n",
      "Iteration 14, loss = 15.68152206\n",
      "Validation score: -71.515574\n",
      "Iteration 15, loss = 320.80100778\n",
      "Validation score: -0.575148\n",
      "Iteration 16, loss = 13.00834775\n",
      "Validation score: -0.271622\n",
      "Iteration 17, loss = 182.82705819\n",
      "Validation score: -113.025431\n",
      "Iteration 18, loss = 451.03268279\n",
      "Validation score: -13.581384\n",
      "Iteration 19, loss = 739.75968031\n",
      "Validation score: -198.897063\n",
      "Iteration 20, loss = 124.39654261\n",
      "Validation score: -7.209386\n",
      "Iteration 21, loss = 1212.31078738\n",
      "Validation score: -1.962124\n",
      "Iteration 22, loss = 106.61458857\n",
      "Validation score: -11.712093\n",
      "Iteration 23, loss = 1155.13966684\n",
      "Validation score: -125.599254\n",
      "Iteration 24, loss = 156.78420819\n",
      "Validation score: -0.618321\n",
      "Iteration 25, loss = 13.72267454\n",
      "Validation score: -0.189404\n",
      "Iteration 26, loss = 52.28790016\n",
      "Validation score: -0.387034\n",
      "Iteration 27, loss = 524.97882092\n",
      "Validation score: -117.022373\n",
      "Iteration 28, loss = 245.44244182\n",
      "Validation score: -7.652365\n",
      "Iteration 29, loss = 319.76945638\n",
      "Validation score: -0.191952\n",
      "Iteration 30, loss = 30.68383926\n",
      "Validation score: -7.564491\n",
      "Iteration 31, loss = 218.73795311\n",
      "Validation score: -35.022966\n",
      "Iteration 32, loss = 88.30301025\n",
      "Validation score: -1.309674\n",
      "Iteration 33, loss = 59.63400748\n",
      "Validation score: -0.349672\n",
      "Iteration 34, loss = 233.74489437\n",
      "Validation score: -0.136052\n",
      "Iteration 35, loss = 171.37505500\n",
      "Validation score: -3.827349\n",
      "Iteration 36, loss = 8.82142380\n",
      "Validation score: -0.217548\n",
      "Iteration 37, loss = 12.48823839\n",
      "Validation score: -4.744421\n",
      "Iteration 38, loss = 61.16148804\n",
      "Validation score: -0.177785\n",
      "Iteration 39, loss = 10.03792214\n",
      "Validation score: -0.314397\n",
      "Iteration 40, loss = 22.23364671\n",
      "Validation score: -2.578330\n",
      "Iteration 41, loss = 75.75990675\n",
      "Validation score: -0.826823\n",
      "Iteration 42, loss = 30.89992757\n",
      "Validation score: -0.953024\n",
      "Iteration 43, loss = 10.02719673\n",
      "Validation score: -0.050574\n",
      "Iteration 44, loss = 30.00839771\n",
      "Validation score: -3.571877\n",
      "Iteration 45, loss = 17.96243629\n",
      "Validation score: -6.684999\n",
      "Iteration 46, loss = 17.21332449\n",
      "Validation score: -1.880443\n",
      "Iteration 47, loss = 25.30228714\n",
      "Validation score: -8.690963\n",
      "Iteration 48, loss = 19.32037614\n",
      "Validation score: -1.022816\n",
      "Iteration 49, loss = 8.71495771\n",
      "Validation score: -3.224476\n",
      "Iteration 50, loss = 14.69391640\n",
      "Validation score: -0.817872\n",
      "Iteration 51, loss = 10.30581264\n",
      "Validation score: -2.061621\n",
      "Iteration 52, loss = 6.97048038\n",
      "Validation score: -0.025567\n",
      "Iteration 53, loss = 39.51099360\n",
      "Validation score: -0.587763\n",
      "Iteration 54, loss = 6.19331591\n",
      "Validation score: -0.049829\n",
      "Iteration 55, loss = 5.79014769\n",
      "Validation score: -0.066721\n",
      "Iteration 56, loss = 5.85787007\n",
      "Validation score: -0.026398\n",
      "Iteration 57, loss = 5.92505067\n",
      "Validation score: -0.076945\n",
      "Iteration 58, loss = 6.31760037\n",
      "Validation score: -0.097663\n",
      "Iteration 59, loss = 6.29597675\n",
      "Validation score: -0.224545\n",
      "Iteration 60, loss = 6.49403274\n",
      "Validation score: -0.013652\n",
      "Iteration 61, loss = 6.52508077\n",
      "Validation score: -0.105592\n",
      "Iteration 62, loss = 5.91604230\n",
      "Validation score: -0.073670\n",
      "Iteration 63, loss = 6.93802578\n",
      "Validation score: -0.379386\n",
      "Iteration 64, loss = 6.14611758\n",
      "Validation score: -0.016688\n",
      "Iteration 65, loss = 6.09089486\n",
      "Validation score: -0.158055\n",
      "Iteration 66, loss = 5.79864482\n",
      "Validation score: -0.094918\n",
      "Iteration 67, loss = 6.15694970\n",
      "Validation score: -0.136107\n",
      "Iteration 68, loss = 6.80126646\n",
      "Validation score: -0.185217\n",
      "Iteration 69, loss = 9.50299979\n",
      "Validation score: -0.201013\n",
      "Iteration 70, loss = 5.34705404\n",
      "Validation score: -0.092450\n",
      "Iteration 71, loss = 5.36087352\n",
      "Validation score: -0.021069\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "RMSLE: 0.473176490023574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    # 예측값과 실제값에 1을 더하고 로그 씌우기\n",
    "    log_pred = np.log1p(y_pred)\n",
    "    log_true = np.log1p(y_true)\n",
    "    \n",
    "    # 제곱 오차 계산\n",
    "    squared_error = (log_pred - log_true) ** 2\n",
    "    \n",
    "    # 평균 제곱 오차 계산\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "    \n",
    "    # 루트 씌우기 (Root Mean Squared Error)\n",
    "    rmsle_score = np.sqrt(mean_squared_error)\n",
    "    \n",
    "    return rmsle_score\n",
    "\n",
    "X = train.drop(target,axis=1)\n",
    "y = train[target]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=3)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# MLP 모델 초기화\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(32, 16), \n",
    "                         max_iter=1000, \n",
    "                         random_state=42, \n",
    "                         batch_size=128, \n",
    "                         solver='adam',\n",
    "                         learning_rate='adaptive',\n",
    "                         learning_rate_init=0.01,\n",
    "                         early_stopping=True,\n",
    "                         validation_fraction=0.1,\n",
    "                         activation='relu',\n",
    "                         alpha=0.0001,\n",
    "                         verbose=1)\n",
    "\n",
    "# 모델 훈련\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# 평가 지표 출력 (예: 평균 제곱 오차)\n",
    "mse = rmsle(y_test, y_pred)\n",
    "print(f'RMSLE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "29456df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1981/1981 [==============================] - 1s 505us/step - loss: 0.4520\n",
      "Epoch 2/30\n",
      "1981/1981 [==============================] - 1s 446us/step - loss: 0.4490\n",
      "Epoch 3/30\n",
      "1981/1981 [==============================] - 1s 447us/step - loss: 0.4489\n",
      "Epoch 4/30\n",
      "1981/1981 [==============================] - 1s 466us/step - loss: 0.4486\n",
      "Epoch 5/30\n",
      "1981/1981 [==============================] - 1s 452us/step - loss: 0.4492\n",
      "Epoch 6/30\n",
      "1981/1981 [==============================] - 1s 444us/step - loss: 0.4491\n",
      "Epoch 7/30\n",
      "1981/1981 [==============================] - 1s 472us/step - loss: 0.4488\n",
      "Epoch 8/30\n",
      "1981/1981 [==============================] - 1s 453us/step - loss: 0.4485\n",
      "Epoch 9/30\n",
      "1981/1981 [==============================] - 1s 448us/step - loss: 0.4488\n",
      "Epoch 10/30\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 0.4488\n",
      "Epoch 11/30\n",
      "1981/1981 [==============================] - 1s 456us/step - loss: 0.4488\n",
      "Epoch 12/30\n",
      "1981/1981 [==============================] - 1s 492us/step - loss: 0.4493\n",
      "Epoch 13/30\n",
      "1981/1981 [==============================] - 1s 473us/step - loss: 0.4488\n",
      "Epoch 14/30\n",
      "1981/1981 [==============================] - 1s 453us/step - loss: 0.4488\n",
      "Epoch 15/30\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 0.4486\n",
      "Epoch 16/30\n",
      "1981/1981 [==============================] - 1s 448us/step - loss: 0.4491\n",
      "Epoch 17/30\n",
      "1981/1981 [==============================] - 1s 449us/step - loss: 0.4489\n",
      "Epoch 18/30\n",
      "1981/1981 [==============================] - 1s 460us/step - loss: 0.4490\n",
      "Epoch 19/30\n",
      "1981/1981 [==============================] - 1s 450us/step - loss: 0.4488\n",
      "Epoch 20/30\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 0.4489\n",
      "Epoch 21/30\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 0.4486\n",
      "Epoch 22/30\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 0.4491\n",
      "Epoch 23/30\n",
      "1981/1981 [==============================] - 1s 448us/step - loss: 0.4490\n",
      "Epoch 24/30\n",
      "1981/1981 [==============================] - 1s 449us/step - loss: 0.4487\n",
      "Epoch 25/30\n",
      "1981/1981 [==============================] - 1s 451us/step - loss: 0.4485\n",
      "Epoch 26/30\n",
      "1981/1981 [==============================] - 1s 453us/step - loss: 0.4488\n",
      "Epoch 27/30\n",
      "1981/1981 [==============================] - 1s 450us/step - loss: 0.4487\n",
      "Epoch 28/30\n",
      "1981/1981 [==============================] - 1s 450us/step - loss: 0.4492\n",
      "Epoch 29/30\n",
      "1981/1981 [==============================] - 1s 460us/step - loss: 0.4493\n",
      "Epoch 30/30\n",
      "1981/1981 [==============================] - 1s 452us/step - loss: 0.4488\n",
      "RMSLE on Test Data: 0.4585389792919159\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 예제 데이터 생성 (적절한 데이터로 대체해야 함)\n",
    "X = train.drop(target,axis=1).astype('float32') \n",
    "y = train[target].astype('float32') \n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링 (표준화)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "# RMSLE 손실 함수 정의\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(y_true) - tf.math.log1p(y_pred))))\n",
    "\n",
    "# 모델 구축\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "learning_rate = 0.02  # 적절한 learning rate를 설정해야 함\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "# # 초기 학습률 설정\n",
    "initial_learning_rate = 0.05\n",
    "\n",
    "# 학습률 스케줄링 설정\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100,  # 학습률을 얼마나 자주 감소시킬지 결정\n",
    "    decay_rate=0.1)     # 감소 비율\n",
    "#     staircase=True)\n",
    "\n",
    "# 옵티마이저에 학습률 스케줄 적용\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=optimizer, loss=rmsle_loss)\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train_scaled, y_train, epochs=30, batch_size=16, verbose=1)\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    # 예측값과 실제값에 1을 더하고 로그 씌우기\n",
    "    log_pred = np.log1p(y_pred)\n",
    "    log_true = np.log1p(y_true)\n",
    "    \n",
    "    # 제곱 오차 계산\n",
    "    squared_error = (log_pred - log_true) ** 2\n",
    "    \n",
    "    # 평균 제곱 오차 계산\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "    \n",
    "    # 루트 씌우기 (Root Mean Squared Error)\n",
    "    rmsle_score = np.sqrt(mean_squared_error)\n",
    "    \n",
    "    return rmsle_score\n",
    "\n",
    "# RMSLE 계산\n",
    "rmsle_score = rmsle(y_test, y_pred)\n",
    "print(f'RMSLE on Test Data: {rmsle_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5b64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "263/263 [==============================] - 0s 671us/step - loss: 0.5996\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 0s 469us/step - loss: 0.4536\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 0s 464us/step - loss: 0.4514\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 0s 417us/step - loss: 0.4477\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 0s 477us/step - loss: 0.4455\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 0s 439us/step - loss: 0.4476\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 0s 428us/step - loss: 0.4427\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 0s 435us/step - loss: 0.4446\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 0s 421us/step - loss: 0.4414\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 0s 435us/step - loss: 0.4415\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 0s 458us/step - loss: 0.4407\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 0s 455us/step - loss: 0.4401\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 0s 446us/step - loss: 0.4407\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 0s 446us/step - loss: 0.4373\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 0s 429us/step - loss: 0.4345\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 0s 442us/step - loss: 0.4338\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 0s 417us/step - loss: 0.4344\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 0s 428us/step - loss: 0.4353\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 0s 427us/step - loss: 0.4338\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 0s 419us/step - loss: 0.4326\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 0s 440us/step - loss: 0.4319\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 0s 450us/step - loss: 0.4310\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 0s 449us/step - loss: 0.4287\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 0s 432us/step - loss: 0.4296\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 0s 432us/step - loss: 0.4279\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 0s 473us/step - loss: 0.4306\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 0s 477us/step - loss: 0.4294\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 0s 433us/step - loss: 0.4279\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 0s 442us/step - loss: 0.4283\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 0s 448us/step - loss: 0.4253\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4266\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 0s 428us/step - loss: 0.4253\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 0s 452us/step - loss: 0.4248\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 0s 451us/step - loss: 0.4241\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 0s 443us/step - loss: 0.4236\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 0s 429us/step - loss: 0.4230\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 0s 456us/step - loss: 0.4234\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 0s 438us/step - loss: 0.4192\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 0s 439us/step - loss: 0.4215\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4198\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 0s 443us/step - loss: 0.4176\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 0s 430us/step - loss: 0.4174\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 0s 453us/step - loss: 0.4157\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.4167\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 0s 439us/step - loss: 0.4169\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.4135\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 0s 438us/step - loss: 0.4121\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 0s 439us/step - loss: 0.4130\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 0s 437us/step - loss: 0.4139\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 0s 445us/step - loss: 0.4120\n",
      "Epoch 1/50\n",
      "263/263 [==============================] - 0s 435us/step - loss: 0.6263\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 0s 603us/step - loss: 0.4533\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 0s 475us/step - loss: 0.4456\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.4431\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 0s 428us/step - loss: 0.4396\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 0s 419us/step - loss: 0.4412\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 0s 418us/step - loss: 0.4406\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 0s 423us/step - loss: 0.4369\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4333\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 0s 450us/step - loss: 0.4342\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 0s 456us/step - loss: 0.4333\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 0s 465us/step - loss: 0.4328\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 0s 450us/step - loss: 0.4326\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 0s 459us/step - loss: 0.4337\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 0s 454us/step - loss: 0.4296\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 0s 459us/step - loss: 0.4290\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 0s 446us/step - loss: 0.4295\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 0s 455us/step - loss: 0.4257\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 0s 438us/step - loss: 0.4255\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4221\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 0s 457us/step - loss: 0.4227\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 0s 442us/step - loss: 0.4209\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 0s 449us/step - loss: 0.4209\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 0s 452us/step - loss: 0.4192\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 0s 450us/step - loss: 0.4193\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 0s 455us/step - loss: 0.4197\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 0s 446us/step - loss: 0.4165\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 0s 457us/step - loss: 0.4155\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 0s 468us/step - loss: 0.4139\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 0s 432us/step - loss: 0.4147\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 0s 440us/step - loss: 0.4134\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 0s 433us/step - loss: 0.4116\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 0s 438us/step - loss: 0.4094\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 0s 434us/step - loss: 0.4100\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 0s 421us/step - loss: 0.4074\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 0s 418us/step - loss: 0.4069\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 0s 437us/step - loss: 0.4037\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 0s 449us/step - loss: 0.4051\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 0s 445us/step - loss: 0.4032\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4027\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 0s 430us/step - loss: 0.4015\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 0s 453us/step - loss: 0.4011\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.3996\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 0s 424us/step - loss: 0.4012\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 0s 433us/step - loss: 0.3984\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.3970\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 0s 434us/step - loss: 0.3962\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/263 [==============================] - 0s 453us/step - loss: 0.3944\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.3976\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 0s 449us/step - loss: 0.3940\n",
      "Epoch 1/50\n",
      "263/263 [==============================] - 0s 430us/step - loss: 0.5229\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 0s 430us/step - loss: 0.4544\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 0s 465us/step - loss: 0.4500\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 0s 435us/step - loss: 0.4459\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 0s 469us/step - loss: 0.4415\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.4442\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 0s 443us/step - loss: 0.4425\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 0s 435us/step - loss: 0.4388\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 0s 443us/step - loss: 0.4390\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 0s 417us/step - loss: 0.4376\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 0s 423us/step - loss: 0.4365\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 0s 425us/step - loss: 0.4374\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 0s 417us/step - loss: 0.4318\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 0s 414us/step - loss: 0.4334\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 0s 417us/step - loss: 0.4318\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 0s 437us/step - loss: 0.4315\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 0s 481us/step - loss: 0.4311\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 0s 422us/step - loss: 0.4303\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 0s 440us/step - loss: 0.4296\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 0s 439us/step - loss: 0.4303\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 0s 421us/step - loss: 0.4293\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 0s 428us/step - loss: 0.4253\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 0s 423us/step - loss: 0.4246\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 0s 437us/step - loss: 0.4220\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 0s 434us/step - loss: 0.4215\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 0s 425us/step - loss: 0.4229\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 0s 417us/step - loss: 0.4217\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 0s 471us/step - loss: 0.4196\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4182\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 0s 419us/step - loss: 0.4188\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 0s 428us/step - loss: 0.4172\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 0s 442us/step - loss: 0.4129\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 0s 433us/step - loss: 0.4116\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 0s 441us/step - loss: 0.4117\n",
      "Epoch 35/50\n",
      "263/263 [==============================] - 0s 470us/step - loss: 0.4143\n",
      "Epoch 36/50\n",
      "263/263 [==============================] - 0s 440us/step - loss: 0.4101\n",
      "Epoch 37/50\n",
      "263/263 [==============================] - 0s 424us/step - loss: 0.4088\n",
      "Epoch 38/50\n",
      "263/263 [==============================] - 0s 436us/step - loss: 0.4090\n",
      "Epoch 39/50\n",
      "263/263 [==============================] - 0s 455us/step - loss: 0.4075\n",
      "Epoch 40/50\n",
      "263/263 [==============================] - 0s 424us/step - loss: 0.4068\n",
      "Epoch 41/50\n",
      "263/263 [==============================] - 0s 470us/step - loss: 0.4063\n",
      "Epoch 42/50\n",
      "263/263 [==============================] - 0s 467us/step - loss: 0.4052\n",
      "Epoch 43/50\n",
      "263/263 [==============================] - 0s 472us/step - loss: 0.4025\n",
      "Epoch 44/50\n",
      "263/263 [==============================] - 0s 439us/step - loss: 0.4003\n",
      "Epoch 45/50\n",
      "263/263 [==============================] - 0s 456us/step - loss: 0.4019\n",
      "Epoch 46/50\n",
      "263/263 [==============================] - 0s 435us/step - loss: 0.4003\n",
      "Epoch 47/50\n",
      "263/263 [==============================] - 0s 426us/step - loss: 0.4046\n",
      "Epoch 48/50\n",
      "263/263 [==============================] - 0s 418us/step - loss: 0.3968\n",
      "Epoch 49/50\n",
      "263/263 [==============================] - 0s 452us/step - loss: 0.3993\n",
      "Epoch 50/50\n",
      "263/263 [==============================] - 0s 422us/step - loss: 0.3964\n",
      "Epoch 1/50\n",
      "263/263 [==============================] - 0s 552us/step - loss: 0.5986\n",
      "Epoch 2/50\n",
      "263/263 [==============================] - 0s 460us/step - loss: 0.4471\n",
      "Epoch 3/50\n",
      "263/263 [==============================] - 0s 566us/step - loss: 0.4462\n",
      "Epoch 4/50\n",
      "263/263 [==============================] - 0s 589us/step - loss: 0.4440\n",
      "Epoch 5/50\n",
      "263/263 [==============================] - 0s 586us/step - loss: 0.4403\n",
      "Epoch 6/50\n",
      "263/263 [==============================] - 0s 573us/step - loss: 0.4376\n",
      "Epoch 7/50\n",
      "263/263 [==============================] - 0s 492us/step - loss: 0.4393\n",
      "Epoch 8/50\n",
      "263/263 [==============================] - 0s 473us/step - loss: 0.4360\n",
      "Epoch 9/50\n",
      "263/263 [==============================] - 0s 483us/step - loss: 0.4329\n",
      "Epoch 10/50\n",
      "263/263 [==============================] - 0s 513us/step - loss: 0.4352\n",
      "Epoch 11/50\n",
      "263/263 [==============================] - 0s 691us/step - loss: 0.4332\n",
      "Epoch 12/50\n",
      "263/263 [==============================] - 0s 470us/step - loss: 0.4309\n",
      "Epoch 13/50\n",
      "263/263 [==============================] - 0s 492us/step - loss: 0.4308\n",
      "Epoch 14/50\n",
      "263/263 [==============================] - 0s 496us/step - loss: 0.4301\n",
      "Epoch 15/50\n",
      "263/263 [==============================] - 0s 447us/step - loss: 0.4324\n",
      "Epoch 16/50\n",
      "263/263 [==============================] - 0s 471us/step - loss: 0.4283\n",
      "Epoch 17/50\n",
      "263/263 [==============================] - 0s 458us/step - loss: 0.4283\n",
      "Epoch 18/50\n",
      "263/263 [==============================] - 0s 456us/step - loss: 0.4271\n",
      "Epoch 19/50\n",
      "263/263 [==============================] - 0s 451us/step - loss: 0.4253\n",
      "Epoch 20/50\n",
      "263/263 [==============================] - 0s 454us/step - loss: 0.4232\n",
      "Epoch 21/50\n",
      "263/263 [==============================] - 0s 463us/step - loss: 0.4220\n",
      "Epoch 22/50\n",
      "263/263 [==============================] - 0s 446us/step - loss: 0.4220\n",
      "Epoch 23/50\n",
      "263/263 [==============================] - 0s 453us/step - loss: 0.4215\n",
      "Epoch 24/50\n",
      "263/263 [==============================] - 0s 449us/step - loss: 0.4189\n",
      "Epoch 25/50\n",
      "263/263 [==============================] - 0s 474us/step - loss: 0.4195\n",
      "Epoch 26/50\n",
      "263/263 [==============================] - 0s 452us/step - loss: 0.4172\n",
      "Epoch 27/50\n",
      "263/263 [==============================] - 0s 433us/step - loss: 0.4173\n",
      "Epoch 28/50\n",
      "263/263 [==============================] - 0s 447us/step - loss: 0.4141\n",
      "Epoch 29/50\n",
      "263/263 [==============================] - 0s 430us/step - loss: 0.4123\n",
      "Epoch 30/50\n",
      "263/263 [==============================] - 0s 430us/step - loss: 0.4136\n",
      "Epoch 31/50\n",
      "263/263 [==============================] - 0s 442us/step - loss: 0.4118\n",
      "Epoch 32/50\n",
      "263/263 [==============================] - 0s 427us/step - loss: 0.4112\n",
      "Epoch 33/50\n",
      "263/263 [==============================] - 0s 539us/step - loss: 0.4079\n",
      "Epoch 34/50\n",
      "263/263 [==============================] - 0s 509us/step - loss: 0.4090\n",
      "Epoch 35/50\n",
      "218/263 [=======================>......] - ETA: 0s - loss: 0.4077"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 예제 데이터 생성 (적절한 데이터로 대체해야 함)\n",
    "X = train.drop(target,axis=1).astype('float32') \n",
    "y = train[target].astype('float32') \n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 나누기\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링 (표준화)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "# X_scaled = scaler.transform(X_test) \n",
    "\n",
    "# RMSLE 손실 함수 정의\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(y_true) - tf.math.log1p(y_pred))))\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    # 예측값과 실제값에 1을 더하고 로그 씌우기\n",
    "    log_pred = np.log1p(y_pred)\n",
    "    log_true = np.log1p(y_true)\n",
    "    \n",
    "    # 제곱 오차 계산\n",
    "    squared_error = (log_pred - log_true) ** 2\n",
    "    \n",
    "    # 평균 제곱 오차 계산\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "    \n",
    "    # 루트 씌우기 (Root Mean Squared Error)\n",
    "    rmsle_score = np.sqrt(mean_squared_error)\n",
    "    \n",
    "    return rmsle_score\n",
    "\n",
    "# K-fold 교차 검증 설정\n",
    "n_splits = 10  # 폴드의 개수\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "# 각 폴드에 대한 결과 저장\n",
    "result = []\n",
    "\n",
    "# K-fold 교차 검증 수행\n",
    "for learning_rate, batch_size in tqdm([[0.001,8],[0.001,16],[0.001,32],\n",
    "                                       [0.005,8],[0.005,16],[0.005,32],\n",
    "                                       [0.01,8],[0.01,16],[0.01,32]]):\n",
    "    rmsle_scores = []\n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # 모델 구축\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_dim=X_scaled.shape[1]),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "        # Adam 옵티마이저 설정\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # 모델 컴파일\n",
    "        model.compile(optimizer=optimizer, loss=rmsle_loss)\n",
    "\n",
    "        # 모델 훈련\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=1)\n",
    "\n",
    "        # 테스트 데이터에 대한 예측\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        # RMSLE 계산 및 결과 저장\n",
    "        rmsle_score = rmsle(y_test, y_pred)\n",
    "        rmsle_scores.append(rmsle_score)\n",
    "\n",
    "    # 결과 출력\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    result.append(average_rmsle)\n",
    "    print(f'Average RMSLE across {n_splits} folds: {average_rmsle}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e62bda38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c50ac2e7c0>]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
