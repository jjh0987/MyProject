{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb4f603",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a50d3a",
   "metadata": {},
   "source": [
    "- 'maximum_speed_limit' 이 랜덤포레스트에서 높은 중요도를 가짐 : 주형 EDA\n",
    "- test 에 속도 40 제한 구역이 없다.\n",
    "- 제한속도별로 랜덤포레스트를 돌린후 다시합치는 계획"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1fcf3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_parquet('./train.parquet')\n",
    "test = pd.read_parquet('./test.parquet')\n",
    "train['base_hour'] = list(train['base_date'] + train['base_hour']/24)\n",
    "test['base_hour'] = list(test['base_date'] + test['base_hour']/24)\n",
    "train = train.drop(['base_date','road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "test = test.drop(['base_date','road_in_use','vehicle_restricted','height_restricted'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c3b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0    1665573\n",
       "50.0    1103682\n",
       "70.0     995077\n",
       "80.0     700334\n",
       "30.0     229761\n",
       "40.0       6790\n",
       "Name: maximum_speed_limit, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"maximum_speed_limit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "56519e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_parquet('./train.parquet')\n",
    "test = pd.read_parquet('./test.parquet')\n",
    "train = train.drop(['base_date','road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "test = test.drop(['base_date','road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "\n",
    "from haversine import haversine, Unit, haversine_vector\n",
    "start = [(i-90,j) for i,j in zip(train['start_longitude'],train['start_latitude'])]\n",
    "end = [(i-90,j) for i,j in zip(train['end_longitude'],train['end_latitude'])]\n",
    "distance = [haversine(i, j, unit=Unit.KILOMETERS) for i,j in zip(start,end)]\n",
    "\n",
    "# train = train.drop(['start_longitude','end_longitude','start_latitude','end_latitude'],axis=1)\n",
    "train['distance'] = distance\n",
    "\n",
    "from haversine import haversine, Unit, haversine_vector\n",
    "start = [(i-90,j) for i,j in zip(test['start_longitude'],test['start_latitude'])]\n",
    "end = [(i-90,j) for i,j in zip(test['end_longitude'],test['end_latitude'])]\n",
    "distance = [haversine(i, j, unit=Unit.KILOMETERS) for i,j in zip(start,end)]\n",
    "\n",
    "# train = train.drop(['start_longitude','end_longitude','start_latitude','end_latitude'],axis=1)\n",
    "test['distance'] = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d6d544a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "train = train.drop(train[(train['target'] < 5) & (train['distance'] > 0.1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "19505095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30\n",
    "train = train.drop(train[(train['target'] < 3) & (train['distance'] > 0.1)].index)\n",
    "train = train.drop(train[train['target'] > 70].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b90ed716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70\n",
    "train = train.drop(train[(train['target'] < 5) & (train['distance'] > 0.1)].index)\n",
    "train = train.drop(train[train['target'] > 90].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "687a822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60\n",
    "train = train.drop(train[(train['target'] < 5) & (train['distance'] > 0.1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c8a2ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50\n",
    "train = train.drop(train[(train['target'] < 3) & (train['distance'] > 0.1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cdb6df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['connect_code'] == 0]\n",
    "train = train[train['multi_linked'] == 0]\n",
    "# train = train[train['maximum_speed_limit'] != 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650db48",
   "metadata": {},
   "source": [
    "- 제한속도별로 랜덤포레스트를 돌린후 다시합치는 계획 : 예시로 속도 80제한 데이터만 추출하여 랜덤포레스트 돌려본다.\n",
    "- \"maximum_speed_limit\" 분리되면 사용되지 않으므로 레이블인코딩 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ff9c233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "str_col = [\"day_of_week\",\"lane_count\",\"road_rating\",\"multi_linked\",\"connect_code\"\n",
    "           ,\"weight_restricted\",\"road_type\",\"start_turn_restricted\",\n",
    "          \"end_turn_restricted\",\"road_name\",\"start_node_name\",\"end_node_name\",\n",
    "          'distance'] # \"maximum_speed_limit\"\n",
    "for i in str_col:\n",
    "    le = LabelEncoder()\n",
    "    le=le.fit(train[i])\n",
    "    train[i]=le.transform(train[i])\n",
    "    \n",
    "    for label in np.unique(test[i]):\n",
    "        if label not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test[i]=le.transform(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "0cd802d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1665347, 19)\n",
      "(1665347,)\n",
      "(108606, 19)\n"
     ]
    }
   ],
   "source": [
    "train = train[train[\"maximum_speed_limit\"]==60]\n",
    "y_train = train['target'] \n",
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "\n",
    "test = test[test[\"maximum_speed_limit\"]==60]\n",
    "test_id = test['id']\n",
    "test = test.drop(['id'], axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "94046472",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X_train,y_train,test_size=0.04,random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "25e2b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.158291626064308\n"
     ]
    }
   ],
   "source": [
    "# 80\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tp = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "tp.fit(X_train,y_train)\n",
    "pred = tp.predict(X_test)\n",
    "mae = mean_absolute_error(pred,y_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "acf8ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0024023702434497\n"
     ]
    }
   ],
   "source": [
    "# # 80\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# tp = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "# tp.fit(X_train,y_train)\n",
    "# pred = tp.predict(X_test)\n",
    "# mae = mean_absolute_error(pred,y_test)\n",
    "# print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3f17001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7514721114984426\n"
     ]
    }
   ],
   "source": [
    "# 70\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tp = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "tp.fit(X_train,y_train)\n",
    "pred = tp.predict(X_test)\n",
    "mae = mean_absolute_error(pred,y_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tp = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "tp.fit(X_train,y_train)\n",
    "pred = tp.predict(X_test)\n",
    "mae = mean_absolute_error(pred,y_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "58557ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.783147963575409\n"
     ]
    }
   ],
   "source": [
    "# 50\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tp = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "tp.fit(X_train,y_train)\n",
    "pred = tp.predict(X_test)\n",
    "mae = mean_absolute_error(pred,y_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b4ccd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.895569257203591\n"
     ]
    }
   ],
   "source": [
    "# 30\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tp = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "tp.fit(X_train,y_train)\n",
    "pred = tp.predict(X_test)\n",
    "mae = mean_absolute_error(pred,y_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "80374cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    }
   ],
   "source": [
    "prediction_table = pd.DataFrame(test_id)\n",
    "prediction_table['target'] = tp.predict(test)\n",
    "prediction_table.to_csv(f'./target70.csv')\n",
    "print(f'save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bce62e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_table['target'] = tp.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7aba683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_table.to_csv('./target80.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# r1 = RandomForestRegressor(random_state=10,criterion='absolute_error')\n",
    "# r2 = CatBoostRegressor(random_state=10)\n",
    "# r3 = ExtraTreesRegressor(random_state=10,criterion='absolute_error')\n",
    "# vote = VotingRegressor([('r1', r1), ('r2', r2), ('r3', r3)])\n",
    "# vote.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51c2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(['road_in_use','vehicle_restricted','height_restricted'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "365d3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "def rf_speed(speed,train,test):\n",
    "#     train = pd.read_parquet('./train.parquet')\n",
    "#     test = pd.read_parquet('./test.parquet')\n",
    "#     train = train.drop(['road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "#     test = test.drop(['road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "\n",
    "#     str_col = [\"base_date\",\"base_hour\",\"day_of_week\",\"lane_count\",\"road_rating\",\"multi_linked\",\"connect_code\"\n",
    "#            ,\"weight_restricted\",\"road_type\",\"start_latitude\",\"start_longitude\",\"start_turn_restricted\",\n",
    "#           \"end_turn_restricted\",\"road_name\",\"start_node_name\",\"end_node_name\",'end_latitude','end_longitude','distance'] # \"maximum_speed_limit\"\n",
    "#     for i in str_col:\n",
    "#         le = LabelEncoder()\n",
    "#         le=le.fit(train[i])\n",
    "#         train[i]=le.transform(train[i])\n",
    "\n",
    "#         for label in np.unique(test[i]):\n",
    "#             if label not in le.classes_: \n",
    "#                 le.classes_ = np.append(le.classes_, label)\n",
    "#         test[i]=le.transform(test[i])\n",
    "\n",
    "#    train = train[train[\"maximum_speed_limit\"]==speed]\n",
    "    y_train = train['target'] \n",
    "    X_train = train.drop(['id', 'target'], axis=1)\n",
    "\n",
    "#    test = test[test[\"maximum_speed_limit\"]==speed]\n",
    "    test_id = test['id']\n",
    "    test = test.drop(['id'], axis=1)\n",
    "\n",
    "    prediction_table = pd.DataFrame(test_id)\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(\n",
    "        X_train,y_train,test_size=0.04,random_state=10\n",
    "    )\n",
    "\n",
    "    r1 = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "    # r2 = CatBoostRegressor(random_state=10)\n",
    "#     r3 = ExtraTreesRegressor(random_state=56)\n",
    "    r4 = lgb.LGBMRegressor(random_state=56,num_leaves=1000)\n",
    "#    vote = VotingRegressor([('r1', r1), ('r3', r3)])\n",
    "\n",
    "#    parms = {'verbose':[True]}\n",
    "#    grid_cv = GridSearchCV(r1,param_grid=parms,cv=6,n_jobs=-1,scoring='neg_mean_absolute_error')# ,param_grid=parms\n",
    "    r1.fit(X_train,y_train)\n",
    "\n",
    "    pred = r1.predict(X_test)\n",
    "    mae = mean_absolute_error(pred,y_test)\n",
    "    print(mae)\n",
    "\n",
    "    prediction_table['target'] = r1.predict(test)\n",
    "    prediction_table.to_csv(f'./target.csv')\n",
    "    print(f'save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5893ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.832672342968108\n",
      "save\n"
     ]
    }
   ],
   "source": [
    "rf_speed(80,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98e91060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "def rf_speed(speed,train,test):\n",
    "#     train = pd.read_parquet('./train.parquet')\n",
    "#     test = pd.read_parquet('./test.parquet')\n",
    "#     train = train.drop(['road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "#     test = test.drop(['road_in_use','vehicle_restricted','height_restricted'],axis=1)\n",
    "\n",
    "#     str_col = [\"base_date\",\"base_hour\",\"day_of_week\",\"lane_count\",\"road_rating\",\"multi_linked\",\"connect_code\"\n",
    "#            ,\"weight_restricted\",\"road_type\",\"start_latitude\",\"start_longitude\",\"start_turn_restricted\",\n",
    "#           \"end_turn_restricted\",\"road_name\",\"start_node_name\",\"end_node_name\",'end_latitude','end_longitude','distance'] # \"maximum_speed_limit\"\n",
    "#     for i in str_col:\n",
    "#         le = LabelEncoder()\n",
    "#         le=le.fit(train[i])\n",
    "#         train[i]=le.transform(train[i])\n",
    "\n",
    "#         for label in np.unique(test[i]):\n",
    "#             if label not in le.classes_: \n",
    "#                 le.classes_ = np.append(le.classes_, label)\n",
    "#         test[i]=le.transform(test[i])\n",
    "\n",
    "    train = train[train[\"maximum_speed_limit\"]==speed]\n",
    "    y_train = train['target'] \n",
    "    X_train = train.drop(['id', 'target'], axis=1)\n",
    "\n",
    "    test = test[test[\"maximum_speed_limit\"]==speed]\n",
    "    test_id = test['id']\n",
    "    test = test.drop(['id'], axis=1)\n",
    "\n",
    "    prediction_table = pd.DataFrame(test_id)\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(\n",
    "        X_train,y_train,test_size=0.04,random_state=10\n",
    "    )\n",
    "\n",
    "    r1 = RandomForestRegressor(random_state=56,bootstrap=1000,oob_score=True)\n",
    "    # r2 = CatBoostRegressor(random_state=10)\n",
    "#     r3 = ExtraTreesRegressor(random_state=56)\n",
    "#    r4 = lgb.LGBMRegressor(random_state=56,num_leaves=1000)\n",
    "#    vote = VotingRegressor([('r1', r1), ('r3', r3)])\n",
    "\n",
    "#    parms = {'verbose':[True]}\n",
    "#    grid_cv = GridSearchCV(r1,param_grid=parms,cv=6,n_jobs=-1,scoring='neg_mean_absolute_error')# ,param_grid=parms\n",
    "    r1.fit(X_train,y_train)\n",
    "\n",
    "    pred = r1.predict(X_test)\n",
    "    mae = mean_absolute_error(pred,y_test)\n",
    "    print(mae)\n",
    "\n",
    "    prediction_table['target'] = r1.predict(test)\n",
    "    prediction_table.to_csv(f'./target{speed}.csv')\n",
    "    print(f'save {speed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f296dd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4733089928057557\n",
      "save 80\n",
      "3.2052399738667203\n",
      "save 70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m rf_speed(\u001b[38;5;241m80\u001b[39m,train,test)\n\u001b[0;32m      2\u001b[0m rf_speed(\u001b[38;5;241m70\u001b[39m,train,test)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrf_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m rf_speed(\u001b[38;5;241m50\u001b[39m,train,test)\n\u001b[0;32m      5\u001b[0m rf_speed(\u001b[38;5;241m30\u001b[39m,train,test)\n",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36mrf_speed\u001b[1;34m(speed, train, test)\u001b[0m\n\u001b[0;32m     47\u001b[0m     r1 \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m,bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,oob_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# r2 = CatBoostRegressor(random_state=10)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     r3 = ExtraTreesRegressor(random_state=56)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#    r4 = lgb.LGBMRegressor(random_state=56,num_leaves=1000)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#    parms = {'verbose':[True]}\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#    grid_cv = GridSearchCV(r1,param_grid=parms,cv=6,n_jobs=-1,scoring='neg_mean_absolute_error')# ,param_grid=parms\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[43mr1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     pred \u001b[38;5;241m=\u001b[39m r1\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     58\u001b[0m     mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(pred,y_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_speed(80,train,test)\n",
    "rf_speed(70,train,test)\n",
    "rf_speed(60,train,test)\n",
    "rf_speed(50,train,test)\n",
    "rf_speed(30,train,test) # voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "852d9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.735016550228073\n",
      "save 80\n",
      "6.306186029948537\n",
      "save 70\n",
      "4.470499486331014\n",
      "save 60\n",
      "4.468930608345594\n",
      "save 50\n",
      "3.8811823179312497\n",
      "save 30\n"
     ]
    }
   ],
   "source": [
    "rf_speed(80,train,test)\n",
    "rf_speed(70,train,test)\n",
    "rf_speed(60,train,test)\n",
    "rf_speed(50,train,test)\n",
    "rf_speed(30,train,test) # voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9f54158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  6.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8218885737843054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save 50\n"
     ]
    }
   ],
   "source": [
    "rf_speed(50,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e530c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  4.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.200650902440005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save 70\n"
     ]
    }
   ],
   "source": [
    "rf_speed(70,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e9fe1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "2 fits failed out of a total of 6.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 189, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 458, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 148, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 248, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 763, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"sklearn\\tree\\_tree.pyx\", line 734, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"sklearn\\tree\\_utils.pyx\", line 36, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 117440512 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\User\\anaconda3\\envs\\git_repo\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  9.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8098008501543013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save 60\n"
     ]
    }
   ],
   "source": [
    "rf_speed(60,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8d3860e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.concat([pd.read_csv('target30.csv'),pd.read_csv('target50.csv'),\n",
    "          pd.read_csv('target60.csv'),pd.read_csv('target70.csv'),\n",
    "          pd.read_csv('target80.csv')]).drop('Unnamed: 0',axis=1)\n",
    "ans = ans.sort_values('id')\n",
    "ans = ans.set_index('id')\n",
    "ans.to_csv('./submit.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
