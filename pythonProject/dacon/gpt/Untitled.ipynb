{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2e115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2fad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/junho/Downloads/open 2/train.csv')\n",
    "\n",
    "label_to_text = {'Sci/Tech':0,'Sports':1,'Business':2,'World':3,'Politics':4,'ESG':5,'Health':6,'Entertainment':7}\n",
    "text = [label_to_text[i] for i in label_to_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4de40d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0c5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/junho/Downloads/open 2/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b0af8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = openai.Model.list()\n",
    "# for model in models['data']:\n",
    "#     print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c59ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691771dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# key = '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'\n",
    "# openai.api_key = key\n",
    "\n",
    "# # models = openai.Model.list()\n",
    "# # for model in models['data']:\n",
    "# #     print(model.id)\n",
    "# gpt_answers = []\n",
    "# for i in tqdm(val.index):\n",
    "#     question = f'{train[\"text\"][i]} when you predict wrapped \"\" article and using category {text} which is close to answer?'\n",
    "#     time.sleep(1)\n",
    "#     response = openai.Completion.create(model=\"text-davinci-002\",\n",
    "#                                         prompt=f\"Q: {question}\\nA:\",\n",
    "#                                         temperature=0.1,\n",
    "#                                         max_tokens=10)\n",
    "\n",
    "#     answer = response.choices[0].text.strip()\n",
    "#     gpt_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f9c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# import transformers\n",
    "\n",
    "# # Load the training data\n",
    "# train_data = pd.read_csv('./train.csv')\n",
    "\n",
    "# # Load the pre-trained BERT model\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# model = transformers.TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # Define the input sequence length\n",
    "# max_seq_length = 128\n",
    "\n",
    "# # Define the batch size and number of training epochs\n",
    "# batch_size = 32\n",
    "# num_epochs = 10\n",
    "\n",
    "\n",
    "# # Preprocess the training data\n",
    "# def preprocess(text):\n",
    "#     # Tokenize the text\n",
    "#     tokens = tokenizer.encode_plus(text, max_length=max_seq_length, padding='max_length', truncation=True,\n",
    "#                                    return_attention_mask=True, return_tensors='tf')\n",
    "\n",
    "#     # Get the input IDs and attention mask\n",
    "#     input_ids = tokens['input_ids']\n",
    "#     attention_mask = tokens['attention_mask']\n",
    "\n",
    "#     return input_ids, attention_mask\n",
    "\n",
    "\n",
    "# # Convert the labels to one-hot encoding\n",
    "# label_map = label_to_text\n",
    "# train_data['label'] = train_data['label'].map(label_map)\n",
    "# train_labels = tf.keras.utils.to_categorical(train_data['label'], num_classes=8)\n",
    "\n",
    "# # Create a TensorFlow dataset\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_data['text'].values, train_labels)).map(\n",
    "#     lambda text, label: (preprocess(text), label)).batch(batch_size)\n",
    "\n",
    "# # Define the fine-tuning model\n",
    "# input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_ids')\n",
    "# attention_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='attention_mask')\n",
    "# bert_output = model([input_ids, attention_mask])\n",
    "# dropout = tf.keras.layers.Dropout(0.1)(bert_output[1])\n",
    "# dense = tf.keras.layers.Dense(8, activation='softmax')(dropout)\n",
    "# model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=dense)\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=2e-5, epsilon=1e-08, decay=0.01)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(train_dataset, epochs=num_epochs)\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# model.save_pretrained('fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ae9cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47399, 5000)\n"
     ]
    }
   ],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_csv('/Users/junho/Downloads/open 2/train.csv')\n",
    "\n",
    "# Extract the features and labels\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Define the TF-IDF vectorizer with appropriate settings\n",
    "tfidf_vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the text data into numerical features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Print the shape of the transformed data\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d95b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_df['text'], train_df['label'], test_size=0.08, random_state=42)\n",
    "\n",
    "# Preprocess the text data using CountVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "train_features = vectorizer.fit_transform(train_data).astype(np.float64)\n",
    "val_features = vectorizer.transform(val_data).astype(np.float64)\n",
    "test_features = vectorizer.transform(test_df['text']).astype(np.float64)\n",
    "\n",
    "# Create a LightGBM classifier and specify its hyperparameters\n",
    "params = {\n",
    "    'iteration':200,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 8,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "clf = lgb.LGBMClassifier(**params)\n",
    "\n",
    "# Train the LightGBM model\n",
    "clf.fit(train_features, train_labels, eval_set=[(val_features, val_labels)], early_stopping_rounds=10)\n",
    "\n",
    "# Evaluate the performance of the model on the validation set\n",
    "val_pred = clf.predict(val_features)\n",
    "val_f1_score = f1_score(val_labels, val_pred, average='macro')\n",
    "print('Validation F1 score:', val_f1_score)\n",
    "\n",
    "# Make predictions on the test data and save to a CSV file\n",
    "test_pred = clf.predict(test_features)\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62718dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.386208\n",
      "[200]\tvalid_0's multi_logloss: 0.329822\n",
      "[300]\tvalid_0's multi_logloss: 0.309423\n",
      "[400]\tvalid_0's multi_logloss: 0.300595\n",
      "[500]\tvalid_0's multi_logloss: 0.296805\n",
      "[600]\tvalid_0's multi_logloss: 0.294519\n",
      "[700]\tvalid_0's multi_logloss: 0.293949\n",
      "Early stopping, best iteration is:\n",
      "[694]\tvalid_0's multi_logloss: 0.293807\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.390389\n",
      "[200]\tvalid_0's multi_logloss: 0.335132\n",
      "[300]\tvalid_0's multi_logloss: 0.314864\n",
      "[400]\tvalid_0's multi_logloss: 0.306089\n",
      "[500]\tvalid_0's multi_logloss: 0.30176\n",
      "[600]\tvalid_0's multi_logloss: 0.301217\n",
      "Early stopping, best iteration is:\n",
      "[587]\tvalid_0's multi_logloss: 0.300711\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.398335\n",
      "[200]\tvalid_0's multi_logloss: 0.345689\n",
      "[300]\tvalid_0's multi_logloss: 0.325026\n",
      "[400]\tvalid_0's multi_logloss: 0.316018\n",
      "[500]\tvalid_0's multi_logloss: 0.312075\n",
      "[600]\tvalid_0's multi_logloss: 0.310893\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's multi_logloss: 0.310774\n",
      "Mean validation accuracy: 0.3018\n",
      "Standard deviation of validation accuracy: 0.0070\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Extract the 'text' and 'label' columns\n",
    "X = train_df['text']\n",
    "y = train_df['label']\n",
    "\n",
    "# Define the TF-IDF vectorizer with appropriate settings\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Define the K-Fold cross-validator with 10 splits\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# Initialize an empty list to store the validation accuracy scores\n",
    "val_acc_scores = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X):\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Fit the vectorizer on the training data and transform the text data into numerical features\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "    # Define the LightGBM classifier model\n",
    "    model = LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='multiclass', \n",
    "                           num_class=8, \n",
    "                           max_depth=6, \n",
    "                           learning_rate=0.1, \n",
    "                           n_estimators=1000, \n",
    "                           n_jobs=-1, \n",
    "                           random_state=42)\n",
    "\n",
    "    # Fit the model on the training set\n",
    "    model.fit(X_train_tfidf, y_train,\n",
    "              eval_set=(X_val_tfidf, y_val),\n",
    "              early_stopping_rounds=100,\n",
    "              verbose=100)\n",
    "\n",
    "    # Compute the validation accuracy score and append it to the list\n",
    "    val_acc_scores.append(model.best_score_['valid_0']['multi_logloss'])\n",
    "\n",
    "# Print the mean and standard deviation of the validation accuracy scores\n",
    "print(f'Mean validation accuracy: {np.mean(val_acc_scores):.4f}')\n",
    "print(f'Standard deviation of validation accuracy: {np.std(val_acc_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1bbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
